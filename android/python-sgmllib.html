        <html><head></head><body>
        <h1>Report for python-feedparser on toolchain
            <a href="index.html">android</a>
        </h1>
        <p>Return code: 5</p>
        <p>Time to run: 00:01:39</p>
        <p>Log output:</p>
        <code><pre><docker run --rm -v /tuscan_data --volumes-from tuscan_data -v /sources --volumes-from sources -v /var/cache/pacman/pkg --volumes-from pkg_cache_android -v logs:/logs make_package_container --sources-directory /sources --shared-directory /tuscan_data python-feedparser
:: Synchronizing package databases...
downloading repo.db...
downloading core.db...
downloading extra.db...
downloading community.db...
Found 2827 packages in cache
===> sudo -u tuscan makepkg --nobuild --syncdeps --skipinteg --skippgpcheck --skipchecksums --noconfirm --nocolor --log --noprogressbar --nocheck
==> Making package: python-feedparser 5.2.1-1 (Sat Nov 14 01:11:02 UTC 2015)
==> Checking runtime dependencies...
==> Checking buildtime dependencies...
==> Installing missing dependencies...
resolving dependencies...
looking for conflicting packages...

Packages (7) python-packaging-15.3-2  python2-packaging-15.3-2  sqlite-3.9.2-1  libxml2-2.9.2-2  python-setuptools-1:18.5-1  python2-2.7.10-2  python2-setuptools-1:18.5-1

Total Installed Size:  86.94 MiB

:: Proceed with installation? [Y/n] 
checking keyring...
checking package integrity...
loading package files...
checking for file conflicts...
checking available disk space...
installing sqlite...
installing python2...
Optional dependencies for python2
    tk: for IDLE
    python2-setuptools [pending]
    python2-pip
installing libxml2...
Optional dependencies for libxml2
    python2: python bindings to libxml [installed]
installing python2-packaging...
installing python2-setuptools...
installing python-packaging...
installing python-setuptools...
==> Retrieving sources...
  -> Downloading feedparser-5.2.1.tar.gz...
==> WARNING: Skipping all source file integrity checks.
==> Extracting sources...
  -> Extracting feedparser-5.2.1.tar.gz with bsdtar
==> Starting prepare()...
RefactoringTool: Skipping optional fixer: buffer
RefactoringTool: Skipping optional fixer: idioms
RefactoringTool: Skipping optional fixer: set_literal
RefactoringTool: Skipping optional fixer: ws_comma
RefactoringTool: Refactored feedparser/feedparser.py
--- feedparser/feedparser.py	(original)
+++ feedparser/feedparser.py	(refactored)
@@ -139,20 +139,21 @@
 import struct
 import time
 import types
-import urllib
-import urllib2
-import urlparse
+import urllib.request, urllib.parse, urllib.error
+import urllib.request, urllib.error, urllib.parse
+import urllib.parse
 import warnings

-from htmlentitydefs import name2codepoint, codepoint2name, entitydefs
+from html.entities import name2codepoint, codepoint2name, entitydefs
+import collections

 try:
     from io import BytesIO as _StringIO
 except ImportError:
     try:
-        from cStringIO import StringIO as _StringIO
+        from io import StringIO as _StringIO
     except ImportError:
-        from StringIO import StringIO as _StringIO
+        from io import StringIO as _StringIO

 # ---------- optional modules (feedparser will work without these, but with reduced functionality) ----------

@@ -270,22 +271,22 @@
 class NonXMLContentType(ThingsNobodyCaresAboutButMe): pass
 class UndeclaredNamespace(Exception): pass

-SUPPORTED_VERSIONS = {'': u'unknown',
-                      'rss090': u'RSS 0.90',
-                      'rss091n': u'RSS 0.91 (Netscape)',
-                      'rss091u': u'RSS 0.91 (Userland)',
-                      'rss092': u'RSS 0.92',
-                      'rss093': u'RSS 0.93',
-                      'rss094': u'RSS 0.94',
-                      'rss20': u'RSS 2.0',
-                      'rss10': u'RSS 1.0',
-                      'rss': u'RSS (unknown version)',
-                      'atom01': u'Atom 0.1',
-                      'atom02': u'Atom 0.2',
-                      'atom03': u'Atom 0.3',
-                      'atom10': u'Atom 1.0',
-                      'atom': u'Atom (unknown version)',
-                      'cdf': u'CDF',
+SUPPORTED_VERSIONS = {'': 'unknown',
+                      'rss090': 'RSS 0.90',
+                      'rss091n': 'RSS 0.91 (Netscape)',
+                      'rss091u': 'RSS 0.91 (Userland)',
+                      'rss092': 'RSS 0.92',
+                      'rss093': 'RSS 0.93',
+                      'rss094': 'RSS 0.94',
+                      'rss20': 'RSS 2.0',
+                      'rss10': 'RSS 1.0',
+                      'rss': 'RSS (unknown version)',
+                      'atom01': 'Atom 0.1',
+                      'atom02': 'Atom 0.2',
+                      'atom03': 'Atom 0.3',
+                      'atom10': 'Atom 1.0',
+                      'atom': 'Atom (unknown version)',
+                      'cdf': 'CDF',
                       }

 class FeedParserDict(dict):
@@ -313,13 +314,13 @@
             try:
                 return dict.__getitem__(self, 'tags')[0]['term']
             except IndexError:
-                raise KeyError, "object doesn't have key 'category'"
+                raise KeyError("object doesn't have key 'category'")
         elif key == 'enclosures':
-            norel = lambda link: FeedParserDict([(name,value) for (name,value) in link.items() if name!='rel'])
-            return [norel(link) for link in dict.__getitem__(self, 'links') if link['rel']==u'enclosure']
+            norel = lambda link: FeedParserDict([(name,value) for (name,value) in list(link.items()) if name!='rel'])
+            return [norel(link) for link in dict.__getitem__(self, 'links') if link['rel']=='enclosure']
         elif key == 'license':
             for link in dict.__getitem__(self, 'links'):
-                if link['rel']==u'license' and 'href' in link:
+                if link['rel']=='license' and 'href' in link:
                     return link['href']
         elif key == 'updated':
             # Temporarily help developers out by keeping the old
@@ -397,51 +398,51 @@
         try:
             return self.__getitem__(key)
         except KeyError:
-            raise AttributeError, "object has no attribute '%s'" % key
+            raise AttributeError("object has no attribute '%s'" % key)

     def __hash__(self):
         return id(self)

 _cp1252 = {
-    128: unichr(8364), # euro sign
-    130: unichr(8218), # single low-9 quotation mark
-    131: unichr( 402), # latin small letter f with hook
-    132: unichr(8222), # double low-9 quotation mark
-    133: unichr(8230), # horizontal ellipsis
-    134: unichr(8224), # dagger
-    135: unichr(8225), # double dagger
-    136: unichr( 710), # modifier letter circumflex accent
-    137: unichr(8240), # per mille sign
-    138: unichr( 352), # latin capital letter s with caron
-    139: unichr(8249), # single left-pointing angle quotation mark
-    140: unichr( 338), # latin capital ligature oe
-    142: unichr( 381), # latin capital letter z with caron
-    145: unichr(8216), # left single quotation mark
-    146: unichr(8217), # right single quotation mark
-    147: unichr(8220), # left double quotation mark
-    148: unichr(8221), # right double quotation mark
-    149: unichr(8226), # bullet
-    150: unichr(8211), # en dash
-    151: unichr(8212), # em dash
-    152: unichr( 732), # small tilde
-    153: unichr(8482), # trade mark sign
-    154: unichr( 353), # latin small letter s with caron
-    155: unichr(8250), # single right-pointing angle quotation mark
-    156: unichr( 339), # latin small ligature oe
-    158: unichr( 382), # latin small letter z with caron
-    159: unichr( 376), # latin capital letter y with diaeresis
+    128: chr(8364), # euro sign
+    130: chr(8218), # single low-9 quotation mark
+    131: chr( 402), # latin small letter f with hook
+    132: chr(8222), # double low-9 quotation mark
+    133: chr(8230), # horizontal ellipsis
+    134: chr(8224), # dagger
+    135: chr(8225), # double dagger
+    136: chr( 710), # modifier letter circumflex accent
+    137: chr(8240), # per mille sign
+    138: chr( 352), # latin capital letter s with caron
+    139: chr(8249), # single left-pointing angle quotation mark
+    140: chr( 338), # latin capital ligature oe
+    142: chr( 381), # latin capital letter z with caron
+    145: chr(8216), # left single quotation mark
+    146: chr(8217), # right single quotation mark
+    147: chr(8220), # left double quotation mark
+    148: chr(8221), # right double quotation mark
+    149: chr(8226), # bullet
+    150: chr(8211), # en dash
+    151: chr(8212), # em dash
+    152: chr( 732), # small tilde
+    153: chr(8482), # trade mark sign
+    154: chr( 353), # latin small letter s with caron
+    155: chr(8250), # single right-pointing angle quotation mark
+    156: chr( 339), # latin small ligature oe
+    158: chr( 382), # latin small letter z with caron
+    159: chr( 376), # latin capital letter y with diaeresis
 }

 _urifixer = re.compile('^([A-Za-z][A-Za-z0-9+-.]*://)(/*)(.*?)')
 def _urljoin(base, uri):
     uri = _urifixer.sub(r'\1\3', uri)
-    if not isinstance(uri, unicode):
+    if not isinstance(uri, str):
         uri = uri.decode('utf-8', 'ignore')
     try:
-        uri = urlparse.urljoin(base, uri)
+        uri = urllib.parse.urljoin(base, uri)
     except ValueError:
-        uri = u''
-    if not isinstance(uri, unicode):
+        uri = ''
+    if not isinstance(uri, str):
         return uri.decode('utf-8', 'ignore')
     return uri

@@ -519,16 +520,16 @@
     can_be_relative_uri = set(['link', 'id', 'wfw_comment', 'wfw_commentrss', 'docs', 'url', 'href', 'comments', 'icon', 'logo'])
     can_contain_relative_uris = set(['content', 'title', 'summary', 'info', 'tagline', 'subtitle', 'copyright', 'rights', 'description'])
     can_contain_dangerous_markup = set(['content', 'title', 'summary', 'info', 'tagline', 'subtitle', 'copyright', 'rights', 'description'])
-    html_types = [u'text/html', u'application/xhtml+xml']
-
-    def __init__(self, baseuri=None, baselang=None, encoding=u'utf-8'):
+    html_types = ['text/html', 'application/xhtml+xml']
+
+    def __init__(self, baseuri=None, baselang=None, encoding='utf-8'):
         if not self._matchnamespaces:
-            for k, v in self.namespaces.items():
+            for k, v in list(self.namespaces.items()):
                 self._matchnamespaces[k.lower()] = v
         self.feeddata = FeedParserDict() # feed-level data
         self.encoding = encoding # character encoding
         self.entries = [] # list of entry-level data
-        self.version = u'' # feed type/version, see SUPPORTED_VERSIONS
+        self.version = '' # feed type/version, see SUPPORTED_VERSIONS
         self.namespacesInUse = {} # dictionary of namespaces defined by the feed

         # the following are used internally to track state;
@@ -553,7 +554,7 @@
         self.elementstack = []
         self.basestack = []
         self.langstack = []
-        self.baseuri = baseuri or u''
+        self.baseuri = baseuri or ''
         self.lang = baselang or None
         self.svgOK = 0
         self.title_depth = -1
@@ -583,7 +584,7 @@
         # strict xml parsers do -- account for this difference
         if isinstance(self, _LooseFeedParser):
             v = v.replace('&amp;', '&')
-            if not isinstance(v, unicode):
+            if not isinstance(v, str):
                 v = v.decode('utf-8')
         return (k, v)

@@ -592,12 +593,12 @@
         self.depth += 1

         # normalize attrs
-        attrs = map(self._normalize_attributes, attrs)
+        attrs = list(map(self._normalize_attributes, attrs))

         # track xml:base and xml:lang
         attrsD = dict(attrs)
         baseuri = attrsD.get('xml:base', attrsD.get('base')) or self.baseuri
-        if not isinstance(baseuri, unicode):
+        if not isinstance(baseuri, str):
             baseuri = baseuri.decode(self.encoding, 'ignore')
         # ensure that self.baseuri is always an absolute URI that
         # uses a whitelisted URI scheme (e.g. not `javscript:`)
@@ -627,13 +628,13 @@
                 self.trackNamespace(None, uri)

         # track inline content
-        if self.incontent and not self.contentparams.get('type', u'xml').endswith(u'xml'):
+        if self.incontent and not self.contentparams.get('type', 'xml').endswith('xml'):
             if tag in ('xhtml:div', 'div'):
                 return # typepad does this 10/2007
             # element declared itself as escaped markup, but it isn't really
-            self.contentparams['type'] = u'application/xhtml+xml'
-        if self.incontent and self.contentparams.get('type') == u'application/xhtml+xml':
-            if tag.find(':') <> -1:
+            self.contentparams['type'] = 'application/xhtml+xml'
+        if self.incontent and self.contentparams.get('type') == 'application/xhtml+xml':
+            if tag.find(':') != -1:
                 prefix, tag = tag.split(':', 1)
                 namespace = self.namespacesInUse.get(prefix, '')
                 if tag=='math' and namespace=='http://www.w3.org/1998/Math/MathML':
@@ -645,7 +646,7 @@
             return self.handle_data('<%s%s>' % (tag, self.strattrs(attrs)), escape=0)

         # match namespaces
-        if tag.find(':') <> -1:
+        if tag.find(':') != -1:
             prefix, suffix = tag.split(':', 1)
         else:
             prefix, suffix = '', tag
@@ -677,7 +678,7 @@

     def unknown_endtag(self, tag):
         # match namespaces
-        if tag.find(':') <> -1:
+        if tag.find(':') != -1:
             prefix, suffix = tag.split(':', 1)
         else:
             prefix, suffix = '', tag
@@ -698,12 +699,12 @@
             self.pop(prefix + suffix)

         # track inline content
-        if self.incontent and not self.contentparams.get('type', u'xml').endswith(u'xml'):
+        if self.incontent and not self.contentparams.get('type', 'xml').endswith('xml'):
             # element declared itself as escaped markup, but it isn't really
             if tag in ('xhtml:div', 'div'):
                 return # typepad does this 10/2007
-            self.contentparams['type'] = u'application/xhtml+xml'
-        if self.incontent and self.contentparams.get('type') == u'application/xhtml+xml':
+            self.contentparams['type'] = 'application/xhtml+xml'
+        if self.incontent and self.contentparams.get('type') == 'application/xhtml+xml':
             tag = tag.split(':')[-1]
             self.handle_data('</%s>' % tag, escape=0)

@@ -731,7 +732,7 @@
                 c = int(ref[1:], 16)
             else:
                 c = int(ref)
-            text = unichr(c).encode('utf-8')
+            text = chr(c).encode('utf-8')
         self.elementstack[-1][2].append(text)

     def handle_entityref(self, ref):
@@ -750,7 +751,7 @@
             except KeyError:
                 text = '&%s;' % ref
             else:
-                text = unichr(name2codepoint[ref]).encode('utf-8')
+                text = chr(name2codepoint[ref]).encode('utf-8')
         self.elementstack[-1][2].append(text)

     def handle_data(self, text, escape=1):
@@ -758,7 +759,7 @@
         # not containing any character or entity references
         if not self.elementstack:
             return
-        if escape and self.contentparams.get('type') == u'application/xhtml+xml':
+        if escape and self.contentparams.get('type') == 'application/xhtml+xml':
             text = _xmlescape(text)
         self.elementstack[-1][2].append(text)

@@ -794,25 +795,25 @@
     def mapContentType(self, contentType):
         contentType = contentType.lower()
         if contentType == 'text' or contentType == 'plain':
-            contentType = u'text/plain'
+            contentType = 'text/plain'
         elif contentType == 'html':
-            contentType = u'text/html'
+            contentType = 'text/html'
         elif contentType == 'xhtml':
-            contentType = u'application/xhtml+xml'
+            contentType = 'application/xhtml+xml'
         return contentType

     def trackNamespace(self, prefix, uri):
         loweruri = uri.lower()
         if not self.version:
             if (prefix, loweruri) == (None, 'http://my.netscape.com/rdf/simple/0.9/'):
-                self.version = u'rss090'
+                self.version = 'rss090'
             elif loweruri == 'http://purl.org/rss/1.0/':
-                self.version = u'rss10'
+                self.version = 'rss10'
             elif loweruri == 'http://www.w3.org/2005/atom':
-                self.version = u'atom10'
-        if loweruri.find(u'backend.userland.com/rss') <> -1:
+                self.version = 'atom10'
+        if loweruri.find('backend.userland.com/rss') != -1:
             # match any backend.userland.com namespace
-            uri = u'http://backend.userland.com/rss'
+            uri = 'http://backend.userland.com/rss'
             loweruri = uri
         if loweruri in self._matchnamespaces:
             self.namespacemap[prefix] = self._matchnamespaces[loweruri]
@@ -821,7 +822,7 @@
             self.namespacesInUse[prefix or ''] = uri

     def resolveURI(self, uri):
-        return _urljoin(self.baseuri or u'', uri)
+        return _urljoin(self.baseuri or '', uri)

     def decodeEntities(self, element, data):
         return data
@@ -840,7 +841,7 @@

         element, expectingText, pieces = self.elementstack.pop()

-        if self.version == u'atom10' and self.contentparams.get('type', u'text') == u'application/xhtml+xml':
+        if self.version == 'atom10' and self.contentparams.get('type', 'text') == 'application/xhtml+xml':
             # remove enclosing child element, but only if it is a <div> and
             # only if all the remaining content is nested underneath it.
             # This means that the divs would be retained in the following:
@@ -863,10 +864,10 @@

         # Ensure each piece is a str for Python 3
         for (i, v) in enumerate(pieces):
-            if not isinstance(v, unicode):
+            if not isinstance(v, str):
                 pieces[i] = v.decode('utf-8')

-        output = u''.join(pieces)
+        output = ''.join(pieces)
         if stripWhitespace:
             output = output.strip()
         if not expectingText:
@@ -897,9 +898,9 @@

         # some feed formats require consumers to guess
         # whether the content is html or plain text
-        if not self.version.startswith(u'atom') and self.contentparams.get('type') == u'text/plain':
+        if not self.version.startswith('atom') and self.contentparams.get('type') == 'text/plain':
             if self.lookslikehtml(output):
-                self.contentparams['type'] = u'text/html'
+                self.contentparams['type'] = 'text/html'

         # remove temporary cruft from contentparams
         try:
@@ -911,30 +912,30 @@
         except KeyError:
             pass

-        is_htmlish = self.mapContentType(self.contentparams.get('type', u'text/html')) in self.html_types
+        is_htmlish = self.mapContentType(self.contentparams.get('type', 'text/html')) in self.html_types
         # resolve relative URIs within embedded markup
         if is_htmlish and RESOLVE_RELATIVE_URIS:
             if element in self.can_contain_relative_uris:
-                output = _resolveRelativeURIs(output, self.baseuri, self.encoding, self.contentparams.get('type', u'text/html'))
+                output = _resolveRelativeURIs(output, self.baseuri, self.encoding, self.contentparams.get('type', 'text/html'))

         # sanitize embedded markup
         if is_htmlish and SANITIZE_HTML:
             if element in self.can_contain_dangerous_markup:
-                output = _sanitizeHTML(output, self.encoding, self.contentparams.get('type', u'text/html'))
-
-        if self.encoding and not isinstance(output, unicode):
+                output = _sanitizeHTML(output, self.encoding, self.contentparams.get('type', 'text/html'))
+
+        if self.encoding and not isinstance(output, str):
             output = output.decode(self.encoding, 'ignore')

         # address common error where people take data that is already
         # utf-8, presume that it is iso-8859-1, and re-encode it.
-        if self.encoding in (u'utf-8', u'utf-8_INVALID_PYTHON_3') and isinstance(output, unicode):
+        if self.encoding in ('utf-8', 'utf-8_INVALID_PYTHON_3') and isinstance(output, str):
             try:
                 output = output.encode('iso-8859-1').decode('utf-8')
             except (UnicodeEncodeError, UnicodeDecodeError):
                 pass

         # map win-1252 extensions to the proper code points
-        if isinstance(output, unicode):
+        if isinstance(output, str):
             output = output.translate(_cp1252)

         # categories/tags/keywords/whatever are handled in _end_category or _end_tags or _end_itunes_keywords
@@ -1016,19 +1017,18 @@
             return

         # all tags must be in a restricted subset of valid HTML tags
-        if filter(lambda t: t.lower() not in _HTMLSanitizer.acceptable_elements,
-            re.findall(r'</?(\w+)',s)):
+        if [t for t in re.findall(r'</?(\w+)',s) if t.lower() not in _HTMLSanitizer.acceptable_elements]:
             return

         # all entities must have been defined as valid HTML entities
-        if filter(lambda e: e not in entitydefs.keys(), re.findall(r'&(\w+);', s)):
+        if [e for e in re.findall(r'&(\w+);', s) if e not in list(entitydefs.keys())]:
             return

         return 1

     def _mapToStandardPrefix(self, name):
         colonpos = name.find(':')
-        if colonpos <> -1:
+        if colonpos != -1:
             prefix = name[:colonpos]
             suffix = name[colonpos+1:]
             prefix = self.namespacemap.get(prefix, prefix)
@@ -1041,11 +1041,11 @@
     def _isBase64(self, attrsD, contentparams):
         if attrsD.get('mode', '') == 'base64':
             return 1
-        if self.contentparams['type'].startswith(u'text/'):
+        if self.contentparams['type'].startswith('text/'):
             return 0
-        if self.contentparams['type'].endswith(u'+xml'):
+        if self.contentparams['type'].endswith('+xml'):
             return 0
-        if self.contentparams['type'].endswith(u'/xml'):
+        if self.contentparams['type'].endswith('/xml'):
             return 0
         return 1

@@ -1071,22 +1071,22 @@
             context.setdefault(key, value)

     def _start_rss(self, attrsD):
-        versionmap = {'0.91': u'rss091u',
-                      '0.92': u'rss092',
-                      '0.93': u'rss093',
-                      '0.94': u'rss094'}
+        versionmap = {'0.91': 'rss091u',
+                      '0.92': 'rss092',
+                      '0.93': 'rss093',
+                      '0.94': 'rss094'}
         #If we're here then this is an RSS feed.
         #If we don't have a version or have a version that starts with something
         #other than RSS then there's been a mistake. Correct it.
-        if not self.version or not self.version.startswith(u'rss'):
+        if not self.version or not self.version.startswith('rss'):
             attr_version = attrsD.get('version', '')
             version = versionmap.get(attr_version)
             if version:
                 self.version = version
             elif attr_version.startswith('2.'):
-                self.version = u'rss20'
+                self.version = 'rss20'
             else:
-                self.version = u'rss'
+                self.version = 'rss'

     def _start_channel(self, attrsD):
         self.infeed = 1
@@ -1104,16 +1104,16 @@

     def _start_feed(self, attrsD):
         self.infeed = 1
-        versionmap = {'0.1': u'atom01',
-                      '0.2': u'atom02',
-                      '0.3': u'atom03'}
+        versionmap = {'0.1': 'atom01',
+                      '0.2': 'atom02',
+                      '0.3': 'atom03'}
         if not self.version:
             attr_version = attrsD.get('version')
             version = versionmap.get(attr_version)
             if version:
                 self.version = version
             else:
-                self.version = u'atom'
+                self.version = 'atom'

     def _end_channel(self):
         self.infeed = 0
@@ -1300,7 +1300,7 @@
             name = detail.get('name')
             email = detail.get('email')
             if name and email:
-                context[key] = u'%s (%s)' % (name, email)
+                context[key] = '%s (%s)' % (name, email)
             elif name:
                 context[key] = name
             elif email:
@@ -1309,18 +1309,18 @@
             author, email = context.get(key), None
             if not author:
                 return
-            emailmatch = re.search(ur'''(([a-zA-Z0-9\_\-\.\+]+)@((\[[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.)|(([a-zA-Z0-9\-]+\.)+))([a-zA-Z]{2,4}|[0-9]{1,3})(\]?))(\?subject=\S+)?''', author)
+            emailmatch = re.search(r'''(([a-zA-Z0-9\_\-\.\+]+)@((\[[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.)|(([a-zA-Z0-9\-]+\.)+))([a-zA-Z]{2,4}|[0-9]{1,3})(\]?))(\?subject=\S+)?''', author)
             if emailmatch:
                 email = emailmatch.group(0)
                 # probably a better way to do the following, but it passes all the tests
-                author = author.replace(email, u'')
-                author = author.replace(u'()', u'')
-                author = author.replace(u'<>', u'')
-                author = author.replace(u'&lt;&gt;', u'')
+                author = author.replace(email, '')
+                author = author.replace('()', '')
+                author = author.replace('<>', '')
+                author = author.replace('&lt;&gt;', '')
                 author = author.strip()
-                if author and (author[0] == u'('):
+                if author and (author[0] == '('):
                     author = author[1:]
-                if author and (author[-1] == u')'):
+                if author and (author[-1] == ')'):
                     author = author[:-1]
                 author = author.strip()
             if author or email:
@@ -1331,7 +1331,7 @@
                 detail['email'] = email

     def _start_subtitle(self, attrsD):
-        self.pushContent('subtitle', attrsD, u'text/plain', 1)
+        self.pushContent('subtitle', attrsD, 'text/plain', 1)
     _start_tagline = _start_subtitle
     _start_itunes_subtitle = _start_subtitle

@@ -1341,7 +1341,7 @@
     _end_itunes_subtitle = _end_subtitle

     def _start_rights(self, attrsD):
-        self.pushContent('rights', attrsD, u'text/plain', 1)
+        self.pushContent('rights', attrsD, 'text/plain', 1)
     _start_dc_rights = _start_rights
     _start_copyright = _start_rights

@@ -1570,7 +1570,7 @@
         context = self._getContext()
         value = self._getAttribute(attrsD, 'rdf:resource')
         attrsD = FeedParserDict()
-        attrsD['rel'] = u'license'
+        attrsD['rel'] = 'license'
         if value:
             attrsD['href']=value
         context.setdefault('links', []).append(attrsD)
@@ -1583,7 +1583,7 @@
         value = self.pop('license')
         context = self._getContext()
         attrsD = FeedParserDict()
-        attrsD['rel'] = u'license'
+        attrsD['rel'] = 'license'
         if value:
             attrsD['href'] = value
         context.setdefault('links', []).append(attrsD)
@@ -1619,13 +1619,13 @@
     _start_keywords = _start_category

     def _start_media_category(self, attrsD):
-        attrsD.setdefault('scheme', u'http://search.yahoo.com/mrss/category_schema')
+        attrsD.setdefault('scheme', 'http://search.yahoo.com/mrss/category_schema')
         self._start_category(attrsD)

     def _end_itunes_keywords(self):
         for term in self.pop('itunes_keywords').split(','):
             if term.strip():
-                self._addTag(term.strip(), u'http://www.itunes.com/', None)
+                self._addTag(term.strip(), 'http://www.itunes.com/', None)

     def _end_media_keywords(self):
         for term in self.pop('media_keywords').split(','):
@@ -1633,7 +1633,7 @@
                 self._addTag(term.strip(), None, None)

     def _start_itunes_category(self, attrsD):
-        self._addTag(attrsD.get('text'), u'http://www.itunes.com/', None)
+        self._addTag(attrsD.get('text'), 'http://www.itunes.com/', None)
         self.push('category', 1)

     def _end_category(self):
@@ -1655,11 +1655,11 @@
         self._getContext()['cloud'] = FeedParserDict(attrsD)

     def _start_link(self, attrsD):
-        attrsD.setdefault('rel', u'alternate')
-        if attrsD['rel'] == u'self':
-            attrsD.setdefault('type', u'application/atom+xml')
+        attrsD.setdefault('rel', 'alternate')
+        if attrsD['rel'] == 'self':
+            attrsD.setdefault('type', 'application/atom+xml')
         else:
-            attrsD.setdefault('type', u'text/html')
+            attrsD.setdefault('type', 'text/html')
         context = self._getContext()
         attrsD = self._itsAnHrefDamnIt(attrsD)
         if 'href' in attrsD:
@@ -1670,7 +1670,7 @@
             context['links'].append(FeedParserDict(attrsD))
         if 'href' in attrsD:
             expectingText = 0
-            if (attrsD.get('rel') == u'alternate') and (self.mapContentType(attrsD.get('type')) in self.html_types):
+            if (attrsD.get('rel') == 'alternate') and (self.mapContentType(attrsD.get('type')) in self.html_types):
                 context['link'] = attrsD['href']
         else:
             self.push('link', expectingText)
@@ -1694,8 +1694,8 @@

     def _start_title(self, attrsD):
         if self.svgOK:
-            return self.unknown_starttag('title', attrsD.items())
-        self.pushContent('title', attrsD, u'text/plain', self.infeed or self.inentry or self.insource)
+            return self.unknown_starttag('title', list(attrsD.items()))
+        self.pushContent('title', attrsD, 'text/plain', self.infeed or self.inentry or self.insource)
     _start_dc_title = _start_title
     _start_media_title = _start_title

@@ -1719,12 +1719,12 @@
             self._summaryKey = 'content'
             self._start_content(attrsD)
         else:
-            self.pushContent('description', attrsD, u'text/html', self.infeed or self.inentry or self.insource)
+            self.pushContent('description', attrsD, 'text/html', self.infeed or self.inentry or self.insource)
     _start_dc_description = _start_description
     _start_media_description = _start_description

     def _start_abstract(self, attrsD):
-        self.pushContent('description', attrsD, u'text/plain', self.infeed or self.inentry or self.insource)
+        self.pushContent('description', attrsD, 'text/plain', self.infeed or self.inentry or self.insource)

     def _end_description(self):
         if self._summaryKey == 'content':
@@ -1737,7 +1737,7 @@
     _end_media_description = _end_description

     def _start_info(self, attrsD):
-        self.pushContent('info', attrsD, u'text/plain', 1)
+        self.pushContent('info', attrsD, 'text/plain', 1)
     _start_feedburner_browserfriendly = _start_info

     def _end_info(self):
@@ -1780,7 +1780,7 @@
             self._start_content(attrsD)
         else:
             self._summaryKey = 'summary'
-            self.pushContent(self._summaryKey, attrsD, u'text/plain', 1)
+            self.pushContent(self._summaryKey, attrsD, 'text/plain', 1)
     _start_itunes_summary = _start_summary

     def _end_summary(self):
@@ -1794,13 +1794,13 @@
     def _start_enclosure(self, attrsD):
         attrsD = self._itsAnHrefDamnIt(attrsD)
         context = self._getContext()
-        attrsD['rel'] = u'enclosure'
+        attrsD['rel'] = 'enclosure'
         context.setdefault('links', []).append(FeedParserDict(attrsD))

     def _start_source(self, attrsD):
         if 'url' in attrsD:
             # This means that we're processing a source element from an RSS 2.0 feed
-            self.sourcedata['href'] = attrsD[u'url']
+            self.sourcedata['href'] = attrsD['url']
         self.push('source', 1)
         self.insource = 1
         self.title_depth = -1
@@ -1814,22 +1814,22 @@
         self.sourcedata.clear()

     def _start_content(self, attrsD):
-        self.pushContent('content', attrsD, u'text/plain', 1)
+        self.pushContent('content', attrsD, 'text/plain', 1)
         src = attrsD.get('src')
         if src:
             self.contentparams['src'] = src
         self.push('content', 1)

     def _start_body(self, attrsD):
-        self.pushContent('content', attrsD, u'application/xhtml+xml', 1)
+        self.pushContent('content', attrsD, 'application/xhtml+xml', 1)
     _start_xhtml_body = _start_body

     def _start_content_encoded(self, attrsD):
-        self.pushContent('content', attrsD, u'text/html', 1)
+        self.pushContent('content', attrsD, 'text/html', 1)
     _start_fullitem = _start_content_encoded

     def _end_content(self):
-        copyToSummary = self.mapContentType(self.contentparams.get('type')) in ([u'text/plain'] + self.html_types)
+        copyToSummary = self.mapContentType(self.contentparams.get('type')) in (['text/plain'] + self.html_types)
         value = self.popContent('content')
         if copyToSummary:
             self._save('summary', value)
@@ -1986,9 +1986,9 @@
         def startElementNS(self, name, qname, attrs):
             namespace, localname = name
             lowernamespace = str(namespace or '').lower()
-            if lowernamespace.find(u'backend.userland.com/rss') <> -1:
+            if lowernamespace.find('backend.userland.com/rss') != -1:
                 # match any backend.userland.com namespace
-                namespace = u'http://backend.userland.com/rss'
+                namespace = 'http://backend.userland.com/rss'
                 lowernamespace = namespace
             if qname and qname.find(':') > 0:
                 givenprefix = qname.split(':')[0]
@@ -1996,7 +1996,7 @@
                 givenprefix = None
             prefix = self._matchnamespaces.get(lowernamespace, givenprefix)
             if givenprefix and (prefix == None or (prefix == '' and lowernamespace == '')) and givenprefix not in self.namespacesInUse:
-                raise UndeclaredNamespace, "'%s' is not associated with a namespace" % givenprefix
+                raise UndeclaredNamespace("'%s' is not associated with a namespace" % givenprefix)
             localname = str(localname).lower()

             # qname implementation is horribly broken in Python 2.1 (it
@@ -2015,12 +2015,12 @@
             if prefix:
                 localname = prefix.lower() + ':' + localname
             elif namespace and not qname: #Expat
-                for name,value in self.namespacesInUse.items():
+                for name,value in list(self.namespacesInUse.items()):
                     if name and value == namespace:
                         localname = name + ':' + localname
                         break

-            for (namespace, attrlocalname), attrvalue in attrs.items():
+            for (namespace, attrlocalname), attrvalue in list(attrs.items()):
                 lowernamespace = (namespace or '').lower()
                 prefix = self._matchnamespaces.get(lowernamespace, '')
                 if prefix:
@@ -2029,7 +2029,7 @@
             for qname in attrs.getQNames():
                 attrsD[str(qname).lower()] = attrs.getValueByQName(qname)
             localname = str(localname).lower()
-            self.unknown_starttag(localname, attrsD.items())
+            self.unknown_starttag(localname, list(attrsD.items()))

         def characters(self, text):
             self.handle_data(text)
@@ -2045,7 +2045,7 @@
             if prefix:
                 localname = prefix + ':' + localname
             elif namespace and not qname: #Expat
-                for name,value in self.namespacesInUse.items():
+                for name,value in list(self.namespacesInUse.items()):
                     if name and value == namespace:
                         localname = name + ':' + localname
                         break
@@ -2095,11 +2095,11 @@
     # they're declared above, not as they're declared in sgmllib.
     def goahead(self, i):
         pass
-    goahead.func_code = sgmllib.SGMLParser.goahead.func_code
+    goahead.__code__ = sgmllib.SGMLParser.goahead.__code__

     def __parse_starttag(self, i):
         pass
-    __parse_starttag.func_code = sgmllib.SGMLParser.parse_starttag.func_code
+    __parse_starttag.__code__ = sgmllib.SGMLParser.parse_starttag.__code__

     def parse_starttag(self,i):
         j = self.__parse_starttag(i)
@@ -2117,9 +2117,9 @@
             bytes
             if bytes is str:
                 raise NameError
-            self.encoding = self.encoding + u'_INVALID_PYTHON_3'
+            self.encoding = self.encoding + '_INVALID_PYTHON_3'
         except NameError:
-            if self.encoding and isinstance(data, unicode):
+            if self.encoding and isinstance(data, str):
                 data = data.encode(self.encoding)
         sgmllib.SGMLParser.feed(self, data)
         sgmllib.SGMLParser.close(self)
@@ -2128,7 +2128,7 @@
         if not attrs:
             return attrs
         # utility method to be called by descendants
-        attrs = dict([(k.lower(), v) for k, v in attrs]).items()
+        attrs = list(dict([(k.lower(), v) for k, v in attrs]).items())
         attrs = [(k, k in ('rel', 'type') and v.lower() or v) for k, v in attrs]
         attrs.sort()
         return attrs
@@ -2144,14 +2144,14 @@
                 value=value.replace('>','&gt;').replace('<','&lt;').replace('"','&quot;')
                 value = self.bare_ampersand.sub("&amp;", value)
                 # thanks to Kevin Marks for this breathtaking hack to deal with (valid) high-bit attribute values in UTF-8 feeds
-                if not isinstance(value, unicode):
+                if not isinstance(value, str):
                     value = value.decode(self.encoding, 'ignore')
                 try:
                     # Currently, in Python 3 the key is already a str, and cannot be decoded again
-                    uattrs.append((unicode(key, self.encoding), value))
+                    uattrs.append((str(key, self.encoding), value))
                 except TypeError:
                     uattrs.append((key, value))
-            strattrs = u''.join([u' %s="%s"' % (key, value) for key, value in uattrs])
+            strattrs = ''.join([' %s="%s"' % (key, value) for key, value in uattrs])
             if self.encoding:
                 try:
                     strattrs = strattrs.encode(self.encoding)
@@ -2269,7 +2269,7 @@
         data = data.replace('&#x22;', '&quot;')
         data = data.replace('&#39;', '&apos;')
         data = data.replace('&#x27;', '&apos;')
-        if not self.contentparams.get('type', u'xml').endswith(u'xml'):
+        if not self.contentparams.get('type', 'xml').endswith('xml'):
             data = data.replace('&lt;', '<')
             data = data.replace('&gt;', '>')
             data = data.replace('&amp;', '&')
@@ -2336,20 +2336,20 @@
 def _makeSafeAbsoluteURI(base, rel=None):
     # bail if ACCEPTABLE_URI_SCHEMES is empty
     if not ACCEPTABLE_URI_SCHEMES:
-        return _urljoin(base, rel or u'')
+        return _urljoin(base, rel or '')
     if not base:
-        return rel or u''
+        return rel or ''
     if not rel:
         try:
-            scheme = urlparse.urlparse(base)[0]
+            scheme = urllib.parse.urlparse(base)[0]
         except ValueError:
-            return u''
+            return ''
         if not scheme or scheme in ACCEPTABLE_URI_SCHEMES:
             return base
-        return u''
+        return ''
     uri = _urljoin(base, rel)
     if uri.strip().split(':', 1)[0] not in ACCEPTABLE_URI_SCHEMES:
-        return u''
+        return ''
     return uri

 class _HTMLSanitizer(_BaseHTMLProcessor):
@@ -2659,7 +2659,7 @@

         # declare xlink namespace, if needed
         if self.mathmlOK or self.svgOK:
-            if filter(lambda (n,v): n.startswith('xlink:'),attrs):
+            if [n_v for n_v in attrs if n_v[0].startswith('xlink:')]:
                 if not ('xmlns:xlink','http://www.w3.org/1999/xlink') in attrs:
                     attrs.append(('xmlns:xlink','http://www.w3.org/1999/xlink'))

@@ -2668,7 +2668,7 @@
             if key in acceptable_attributes:
                 key=keymap.get(key,key)
                 # make sure the uri uses an acceptable uri scheme
-                if key == u'href':
+                if key == 'href':
                     value = _makeSafeAbsoluteURI(value)
                 clean_attrs.append((key,value))
             elif key=='style':
@@ -2754,7 +2754,7 @@
     data = data.strip().replace('\r\n', '\n')
     return data

-class _FeedURLHandler(urllib2.HTTPDigestAuthHandler, urllib2.HTTPRedirectHandler, urllib2.HTTPDefaultErrorHandler):
+class _FeedURLHandler(urllib.request.HTTPDigestAuthHandler, urllib.request.HTTPRedirectHandler, urllib.request.HTTPDefaultErrorHandler):
     def http_error_default(self, req, fp, code, msg, headers):
         # The default implementation just raises HTTPError.
         # Forget that.
@@ -2762,7 +2762,7 @@
         return fp

     def http_error_301(self, req, fp, code, msg, hdrs):
-        result = urllib2.HTTPRedirectHandler.http_error_301(self, req, fp,
+        result = urllib.request.HTTPRedirectHandler.http_error_301(self, req, fp,
                                                             code, msg, hdrs)
         result.status = code
         result.newurl = result.geturl()
@@ -2785,7 +2785,7 @@
         # header the server sent back (for the realm) and retry
         # the request with the appropriate digest auth headers instead.
         # This evil genius hack has been brought to you by Aaron Swartz.
-        host = urlparse.urlparse(req.get_full_url())[1]
+        host = urllib.parse.urlparse(req.get_full_url())[1]
         if base64 is None or 'Authorization' not in req.headers \
                           or 'WWW-Authenticate' not in headers:
             return self.http_error_default(req, fp, code, msg, headers)
@@ -2834,8 +2834,8 @@
     if hasattr(url_file_stream_or_string, 'read'):
         return url_file_stream_or_string

-    if isinstance(url_file_stream_or_string, basestring) \
-       and urlparse.urlparse(url_file_stream_or_string)[0] in ('http', 'https', 'ftp', 'file', 'feed'):
+    if isinstance(url_file_stream_or_string, str) \
+       and urllib.parse.urlparse(url_file_stream_or_string)[0] in ('http', 'https', 'ftp', 'file', 'feed'):
         # Deal with the feed URI scheme
         if url_file_stream_or_string.startswith('feed:http'):
             url_file_stream_or_string = url_file_stream_or_string[5:]
@@ -2846,21 +2846,21 @@
         # Test for inline user:password credentials for HTTP basic auth
         auth = None
         if base64 and not url_file_stream_or_string.startswith('ftp:'):
-            urltype, rest = urllib.splittype(url_file_stream_or_string)
-            realhost, rest = urllib.splithost(rest)
+            urltype, rest = urllib.parse.splittype(url_file_stream_or_string)
+            realhost, rest = urllib.parse.splithost(rest)
             if realhost:
-                user_passwd, realhost = urllib.splituser(realhost)
+                user_passwd, realhost = urllib.parse.splituser(realhost)
                 if user_passwd:
                     url_file_stream_or_string = '%s://%s%s' % (urltype, realhost, rest)
                     auth = base64.standard_b64encode(user_passwd).strip()

         # iri support
-        if isinstance(url_file_stream_or_string, unicode):
+        if isinstance(url_file_stream_or_string, str):
             url_file_stream_or_string = _convert_to_idn(url_file_stream_or_string)

         # try to open with urllib2 (to use optional headers)
         request = _build_urllib2_request(url_file_stream_or_string, agent, etag, modified, referrer, auth, request_headers)
-        opener = urllib2.build_opener(*tuple(handlers + [_FeedURLHandler()]))
+        opener = urllib.request.build_opener(*tuple(handlers + [_FeedURLHandler()]))
         opener.addheaders = [] # RMK - must clear so we only send our custom User-Agent
         try:
             return opener.open(request)
@@ -2881,7 +2881,7 @@
         pass

     # treat url_file_stream_or_string as string
-    if isinstance(url_file_stream_or_string, unicode):
+    if isinstance(url_file_stream_or_string, str):
         return _StringIO(url_file_stream_or_string.encode('utf-8'))
     return _StringIO(url_file_stream_or_string)

@@ -2890,14 +2890,14 @@
     # this function should only be called with a unicode string
     # strategy: if the host cannot be encoded in ascii, then
     # it'll be necessary to encode it in idn form
-    parts = list(urlparse.urlsplit(url))
+    parts = list(urllib.parse.urlsplit(url))
     try:
         parts[1].encode('ascii')
     except UnicodeEncodeError:
         # the url needs to be converted to idn notation
         host = parts[1].rsplit(':', 1)
         newhost = []
-        port = u''
+        port = ''
         if len(host) == 2:
             port = host.pop()
         for h in host[0].split('.'):
@@ -2905,16 +2905,16 @@
         parts[1] = '.'.join(newhost)
         if port:
             parts[1] += ':' + port
-        return urlparse.urlunsplit(parts)
+        return urllib.parse.urlunsplit(parts)
     else:
         return url

 def _build_urllib2_request(url, agent, etag, modified, referrer, auth, request_headers):
-    request = urllib2.Request(url)
+    request = urllib.request.Request(url)
     request.add_header('User-Agent', agent)
     if etag:
         request.add_header('If-None-Match', etag)
-    if isinstance(modified, basestring):
+    if isinstance(modified, str):
         modified = _parse_date(modified)
     elif isinstance(modified, datetime.datetime):
         modified = modified.utctimetuple()
@@ -2942,7 +2942,7 @@
         request.add_header('Accept', ACCEPT_HEADER)
     # use this for whatever -- cookies, special headers, etc
     # [('Cookie','Something'),('x-special-header','Another Value')]
-    for header_name, header_value in request_headers.items():
+    for header_name, header_value in list(request_headers.items()):
         request.add_header(header_name, header_value)
     request.add_header('A-IM', 'feed') # RFC 3229 support
     return request
@@ -3081,17 +3081,17 @@
 registerDateHandler(_parse_date_iso8601)

 # 8-bit date handling routines written by ytrewq1.
-_korean_year  = u'\ub144' # b3e2 in euc-kr
-_korean_month = u'\uc6d4' # bff9 in euc-kr
-_korean_day   = u'\uc77c' # c0cf in euc-kr
-_korean_am    = u'\uc624\uc804' # bfc0 c0fc in euc-kr
-_korean_pm    = u'\uc624\ud6c4' # bfc0 c8c4 in euc-kr
+_korean_year  = '\ub144' # b3e2 in euc-kr
+_korean_month = '\uc6d4' # bff9 in euc-kr
+_korean_day   = '\uc77c' # c0cf in euc-kr
+_korean_am    = '\uc624\uc804' # bfc0 c0fc in euc-kr
+_korean_pm    = '\uc624\ud6c4' # bfc0 c8c4 in euc-kr

 _korean_onblog_date_re = \
     re.compile('(\d{4})%s\s+(\d{2})%s\s+(\d{2})%s\s+(\d{2}):(\d{2}):(\d{2})' % \
                (_korean_year, _korean_month, _korean_day))
 _korean_nate_date_re = \
-    re.compile(u'(\d{4})-(\d{2})-(\d{2})\s+(%s|%s)\s+(\d{,2}):(\d{,2}):(\d{,2})' % \
+    re.compile('(\d{4})-(\d{2})-(\d{2})\s+(%s|%s)\s+(\d{,2}):(\d{,2}):(\d{,2})' % \
                (_korean_am, _korean_pm))
 def _parse_date_onblog(dateString):
     '''Parse a string according to the OnBlog 8-bit date format'''
@@ -3127,40 +3127,40 @@
 # Unicode strings for Greek date strings
 _greek_months = \
   { \
-   u'\u0399\u03b1\u03bd': u'Jan',       # c9e1ed in iso-8859-7
-   u'\u03a6\u03b5\u03b2': u'Feb',       # d6e5e2 in iso-8859-7
-   u'\u039c\u03ac\u03ce': u'Mar',       # ccdcfe in iso-8859-7
-   u'\u039c\u03b1\u03ce': u'Mar',       # cce1fe in iso-8859-7
-   u'\u0391\u03c0\u03c1': u'Apr',       # c1f0f1 in iso-8859-7
-   u'\u039c\u03ac\u03b9': u'May',       # ccdce9 in iso-8859-7
-   u'\u039c\u03b1\u03ca': u'May',       # cce1fa in iso-8859-7
-   u'\u039c\u03b1\u03b9': u'May',       # cce1e9 in iso-8859-7
-   u'\u0399\u03bf\u03cd\u03bd': u'Jun', # c9effded in iso-8859-7
-   u'\u0399\u03bf\u03bd': u'Jun',       # c9efed in iso-8859-7
-   u'\u0399\u03bf\u03cd\u03bb': u'Jul', # c9effdeb in iso-8859-7
-   u'\u0399\u03bf\u03bb': u'Jul',       # c9f9eb in iso-8859-7
-   u'\u0391\u03cd\u03b3': u'Aug',       # c1fde3 in iso-8859-7
-   u'\u0391\u03c5\u03b3': u'Aug',       # c1f5e3 in iso-8859-7
-   u'\u03a3\u03b5\u03c0': u'Sep',       # d3e5f0 in iso-8859-7
-   u'\u039f\u03ba\u03c4': u'Oct',       # cfeaf4 in iso-8859-7
-   u'\u039d\u03bf\u03ad': u'Nov',       # cdefdd in iso-8859-7
-   u'\u039d\u03bf\u03b5': u'Nov',       # cdefe5 in iso-8859-7
-   u'\u0394\u03b5\u03ba': u'Dec',       # c4e5ea in iso-8859-7
+   '\u0399\u03b1\u03bd': 'Jan',       # c9e1ed in iso-8859-7
+   '\u03a6\u03b5\u03b2': 'Feb',       # d6e5e2 in iso-8859-7
+   '\u039c\u03ac\u03ce': 'Mar',       # ccdcfe in iso-8859-7
+   '\u039c\u03b1\u03ce': 'Mar',       # cce1fe in iso-8859-7
+   '\u0391\u03c0\u03c1': 'Apr',       # c1f0f1 in iso-8859-7
+   '\u039c\u03ac\u03b9': 'May',       # ccdce9 in iso-8859-7
+   '\u039c\u03b1\u03ca': 'May',       # cce1fa in iso-8859-7
+   '\u039c\u03b1\u03b9': 'May',       # cce1e9 in iso-8859-7
+   '\u0399\u03bf\u03cd\u03bd': 'Jun', # c9effded in iso-8859-7
+   '\u0399\u03bf\u03bd': 'Jun',       # c9efed in iso-8859-7
+   '\u0399\u03bf\u03cd\u03bb': 'Jul', # c9effdeb in iso-8859-7
+   '\u0399\u03bf\u03bb': 'Jul',       # c9f9eb in iso-8859-7
+   '\u0391\u03cd\u03b3': 'Aug',       # c1fde3 in iso-8859-7
+   '\u0391\u03c5\u03b3': 'Aug',       # c1f5e3 in iso-8859-7
+   '\u03a3\u03b5\u03c0': 'Sep',       # d3e5f0 in iso-8859-7
+   '\u039f\u03ba\u03c4': 'Oct',       # cfeaf4 in iso-8859-7
+   '\u039d\u03bf\u03ad': 'Nov',       # cdefdd in iso-8859-7
+   '\u039d\u03bf\u03b5': 'Nov',       # cdefe5 in iso-8859-7
+   '\u0394\u03b5\u03ba': 'Dec',       # c4e5ea in iso-8859-7
   }

 _greek_wdays = \
   { \
-   u'\u039a\u03c5\u03c1': u'Sun', # caf5f1 in iso-8859-7
-   u'\u0394\u03b5\u03c5': u'Mon', # c4e5f5 in iso-8859-7
-   u'\u03a4\u03c1\u03b9': u'Tue', # d4f1e9 in iso-8859-7
-   u'\u03a4\u03b5\u03c4': u'Wed', # d4e5f4 in iso-8859-7
-   u'\u03a0\u03b5\u03bc': u'Thu', # d0e5ec in iso-8859-7
-   u'\u03a0\u03b1\u03c1': u'Fri', # d0e1f1 in iso-8859-7
-   u'\u03a3\u03b1\u03b2': u'Sat', # d3e1e2 in iso-8859-7
+   '\u039a\u03c5\u03c1': 'Sun', # caf5f1 in iso-8859-7
+   '\u0394\u03b5\u03c5': 'Mon', # c4e5f5 in iso-8859-7
+   '\u03a4\u03c1\u03b9': 'Tue', # d4f1e9 in iso-8859-7
+   '\u03a4\u03b5\u03c4': 'Wed', # d4e5f4 in iso-8859-7
+   '\u03a0\u03b5\u03bc': 'Thu', # d0e5ec in iso-8859-7
+   '\u03a0\u03b1\u03c1': 'Fri', # d0e1f1 in iso-8859-7
+   '\u03a3\u03b1\u03b2': 'Sat', # d3e1e2 in iso-8859-7
   }

 _greek_date_format_re = \
-    re.compile(u'([^,]+),\s+(\d{2})\s+([^\s]+)\s+(\d{4})\s+(\d{2}):(\d{2}):(\d{2})\s+([^\s]+)')
+    re.compile('([^,]+),\s+(\d{2})\s+([^\s]+)\s+(\d{4})\s+(\d{2}):(\d{2}):(\d{2})\s+([^\s]+)')

 def _parse_date_greek(dateString):
     '''Parse a string according to a Greek 8-bit date format.'''
@@ -3179,22 +3179,22 @@
 # Unicode strings for Hungarian date strings
 _hungarian_months = \
   { \
-    u'janu\u00e1r':   u'01',  # e1 in iso-8859-2
-    u'febru\u00e1ri': u'02',  # e1 in iso-8859-2
-    u'm\u00e1rcius':  u'03',  # e1 in iso-8859-2
-    u'\u00e1prilis':  u'04',  # e1 in iso-8859-2
-    u'm\u00e1ujus':   u'05',  # e1 in iso-8859-2
-    u'j\u00fanius':   u'06',  # fa in iso-8859-2
-    u'j\u00falius':   u'07',  # fa in iso-8859-2
-    u'augusztus':     u'08',
-    u'szeptember':    u'09',
-    u'okt\u00f3ber':  u'10',  # f3 in iso-8859-2
-    u'november':      u'11',
-    u'december':      u'12',
+    'janu\u00e1r':   '01',  # e1 in iso-8859-2
+    'febru\u00e1ri': '02',  # e1 in iso-8859-2
+    'm\u00e1rcius':  '03',  # e1 in iso-8859-2
+    '\u00e1prilis':  '04',  # e1 in iso-8859-2
+    'm\u00e1ujus':   '05',  # e1 in iso-8859-2
+    'j\u00fanius':   '06',  # fa in iso-8859-2
+    'j\u00falius':   '07',  # fa in iso-8859-2
+    'augusztus':     '08',
+    'szeptember':    '09',
+    'okt\u00f3ber':  '10',  # f3 in iso-8859-2
+    'november':      '11',
+    'december':      '12',
   }

 _hungarian_date_format_re = \
-  re.compile(u'(\d{4})-([^-]+)-(\d{,2})T(\d{,2}):(\d{2})((\+|-)(\d{,2}:\d{2}))')
+  re.compile('(\d{4})-([^-]+)-(\d{,2})T(\d{,2}):(\d{2})((\+|-)(\d{,2}:\d{2}))')

 def _parse_date_hungarian(dateString):
     '''Parse a string according to a Hungarian 8-bit date format.'''
@@ -3357,7 +3357,7 @@
     timeparts = parts[3].split(':')
     timeparts = timeparts + ([0] * (3 - len(timeparts)))
     try:
-        (hour, minute, second) = map(int, timeparts)
+        (hour, minute, second) = list(map(int, timeparts))
     except ValueError:
         return None
     tzhour = 0
@@ -3524,9 +3524,9 @@
     # you should definitely install it if you can.
     # http://cjkpython.i18n.org/

-    bom_encoding = u''
-    xml_encoding = u''
-    rfc3023_encoding = u''
+    bom_encoding = ''
+    xml_encoding = ''
+    rfc3023_encoding = ''

     # Look at the first few bytes of the document to guess what
     # its encoding may be. We only need to decode enough of the
@@ -3536,31 +3536,31 @@
     # http://www.w3.org/TR/REC-xml/#sec-guessing-no-ext-info
     # Check for BOMs first.
     if data[:4] == codecs.BOM_UTF32_BE:
-        bom_encoding = u'utf-32be'
+        bom_encoding = 'utf-32be'
         data = data[4:]
     elif data[:4] == codecs.BOM_UTF32_LE:
-        bom_encoding = u'utf-32le'
+        bom_encoding = 'utf-32le'
         data = data[4:]
     elif data[:2] == codecs.BOM_UTF16_BE and data[2:4] != ZERO_BYTES:
-        bom_encoding = u'utf-16be'
+        bom_encoding = 'utf-16be'
         data = data[2:]
     elif data[:2] == codecs.BOM_UTF16_LE and data[2:4] != ZERO_BYTES:
-        bom_encoding = u'utf-16le'
+        bom_encoding = 'utf-16le'
         data = data[2:]
     elif data[:3] == codecs.BOM_UTF8:
-        bom_encoding = u'utf-8'
+        bom_encoding = 'utf-8'
         data = data[3:]
     # Check for the characters '<?xm' in several encodings.
     elif data[:4] == EBCDIC_MARKER:
-        bom_encoding = u'cp037'
+        bom_encoding = 'cp037'
     elif data[:4] == UTF16BE_MARKER:
-        bom_encoding = u'utf-16be'
+        bom_encoding = 'utf-16be'
     elif data[:4] == UTF16LE_MARKER:
-        bom_encoding = u'utf-16le'
+        bom_encoding = 'utf-16le'
     elif data[:4] == UTF32BE_MARKER:
-        bom_encoding = u'utf-32be'
+        bom_encoding = 'utf-32be'
     elif data[:4] == UTF32LE_MARKER:
-        bom_encoding = u'utf-32le'
+        bom_encoding = 'utf-32le'

     tempdata = data
     try:
@@ -3578,10 +3578,10 @@
         xml_encoding = xml_encoding_match.groups()[0].decode('utf-8').lower()
         # Normalize the xml_encoding if necessary.
         if bom_encoding and (xml_encoding in (
-            u'u16', u'utf-16', u'utf16', u'utf_16',
-            u'u32', u'utf-32', u'utf32', u'utf_32',
-            u'iso-10646-ucs-2', u'iso-10646-ucs-4',
-            u'csucs4', u'csunicode', u'ucs-2', u'ucs-4'
+            'u16', 'utf-16', 'utf16', 'utf_16',
+            'u32', 'utf-32', 'utf32', 'utf_32',
+            'iso-10646-ucs-2', 'iso-10646-ucs-4',
+            'csucs4', 'csunicode', 'ucs-2', 'ucs-4'
         )):
             xml_encoding = bom_encoding

@@ -3593,35 +3593,35 @@
     http_content_type = http_headers.get('content-type') or ''
     http_content_type, params = cgi.parse_header(http_content_type)
     http_encoding = params.get('charset', '').replace("'", "")
-    if not isinstance(http_encoding, unicode):
+    if not isinstance(http_encoding, str):
         http_encoding = http_encoding.decode('utf-8', 'ignore')

     acceptable_content_type = 0
-    application_content_types = (u'application/xml', u'application/xml-dtd',
-                                 u'application/xml-external-parsed-entity')
-    text_content_types = (u'text/xml', u'text/xml-external-parsed-entity')
+    application_content_types = ('application/xml', 'application/xml-dtd',
+                                 'application/xml-external-parsed-entity')
+    text_content_types = ('text/xml', 'text/xml-external-parsed-entity')
     if (http_content_type in application_content_types) or \
-       (http_content_type.startswith(u'application/') and
-        http_content_type.endswith(u'+xml')):
+       (http_content_type.startswith('application/') and
+        http_content_type.endswith('+xml')):
         acceptable_content_type = 1
-        rfc3023_encoding = http_encoding or xml_encoding or u'utf-8'
+        rfc3023_encoding = http_encoding or xml_encoding or 'utf-8'
     elif (http_content_type in text_content_types) or \
-         (http_content_type.startswith(u'text/') and
-          http_content_type.endswith(u'+xml')):
+         (http_content_type.startswith('text/') and
+          http_content_type.endswith('+xml')):
         acceptable_content_type = 1
-        rfc3023_encoding = http_encoding or u'us-ascii'
-    elif http_content_type.startswith(u'text/'):
-        rfc3023_encoding = http_encoding or u'us-ascii'
+        rfc3023_encoding = http_encoding or 'us-ascii'
+    elif http_content_type.startswith('text/'):
+        rfc3023_encoding = http_encoding or 'us-ascii'
     elif http_headers and 'content-type' not in http_headers:
-        rfc3023_encoding = xml_encoding or u'iso-8859-1'
+        rfc3023_encoding = xml_encoding or 'iso-8859-1'
     else:
-        rfc3023_encoding = xml_encoding or u'utf-8'
+        rfc3023_encoding = xml_encoding or 'utf-8'
     # gb18030 is a superset of gb2312, so always replace gb2312
     # with gb18030 for greater compatibility.
-    if rfc3023_encoding.lower() == u'gb2312':
-        rfc3023_encoding = u'gb18030'
-    if xml_encoding.lower() == u'gb2312':
-        xml_encoding = u'gb18030'
+    if rfc3023_encoding.lower() == 'gb2312':
+        rfc3023_encoding = 'gb18030'
+    if xml_encoding.lower() == 'gb2312':
+        xml_encoding = 'gb18030'

     # there are four encodings to keep track of:
     # - http_encoding is the encoding declared in the Content-Type HTTP header
@@ -3646,13 +3646,13 @@
             chardet_encoding = chardet.detect(data)['encoding']
             if not chardet_encoding:
                 chardet_encoding = ''
-            if not isinstance(chardet_encoding, unicode):
-                chardet_encoding = unicode(chardet_encoding, 'ascii', 'ignore')
+            if not isinstance(chardet_encoding, str):
+                chardet_encoding = str(chardet_encoding, 'ascii', 'ignore')
             return chardet_encoding
     # try: HTTP encoding, declared XML encoding, encoding sniffed from BOM
     for proposed_encoding in (rfc3023_encoding, xml_encoding, bom_encoding,
-                              lazy_chardet_encoding, u'utf-8', u'windows-1252', u'iso-8859-2'):
-        if callable(proposed_encoding):
+                              lazy_chardet_encoding, 'utf-8', 'windows-1252', 'iso-8859-2'):
+        if isinstance(proposed_encoding, collections.Callable):
             proposed_encoding = proposed_encoding()
         if not proposed_encoding:
             continue
@@ -3670,7 +3670,7 @@
             if RE_XML_DECLARATION.search(data):
                 data = RE_XML_DECLARATION.sub(new_declaration, data)
             else:
-                data = new_declaration + u'\n' + data
+                data = new_declaration + '\n' + data
RefactoringTool: Refactored feedparser/feedparsertest.py
             data = data.encode('utf-8')
             break
     # if still no luck, give up
@@ -3679,7 +3679,7 @@
             'document encoding unknown, I tried ' +
             '%s, %s, utf-8, windows-1252, and iso-8859-2 but nothing worked' %
             (rfc3023_encoding, xml_encoding))
-        rfc3023_encoding = u''
+        rfc3023_encoding = ''
     elif proposed_encoding != rfc3023_encoding:
         error = CharacterEncodingOverride(
             'document declared as %s, but parsed as %s' %
@@ -3725,7 +3725,7 @@
     doctype_results = RE_DOCTYPE_PATTERN.findall(head)
     doctype = doctype_results and doctype_results[0] or _s2bytes('')
     if _s2bytes('netscape') in doctype.lower():
-        version = u'rss091n'
+        version = 'rss091n'
     else:
         version = None

@@ -3733,7 +3733,7 @@
     replacement = _s2bytes('')
     if len(doctype_results) == 1 and entity_results:
         match_safe_entities = lambda e: RE_SAFE_ENTITY_PATTERN.match(e)
-        safe_entities = filter(match_safe_entities, entity_results)
+        safe_entities = list(filter(match_safe_entities, entity_results))
         if safe_entities:
             replacement = _s2bytes('<!DOCTYPE feed [\n<!ENTITY') \
                         + _s2bytes('>\n<!ENTITY ').join(safe_entities) \
@@ -3754,15 +3754,15 @@
         return _parse_georss_line(value, swap, dims)
     elif geom_type == 'polygon':
         ring = _parse_georss_line(value, swap, dims)
-        return {'type': u'Polygon', 'coordinates': (ring['coordinates'],)}
+        return {'type': 'Polygon', 'coordinates': (ring['coordinates'],)}
     else:
         return None

 def _gen_georss_coords(value, swap=True, dims=2):
     # A generator of (lon, lat) pairs from a string of encoded GeoRSS
     # coordinates. Converts to floats and swaps order.
-    latlons = itertools.imap(float, value.strip().replace(',', ' ').split())
-    nxt = latlons.next
+    latlons = map(float, value.strip().replace(',', ' ').split())
+    nxt = latlons.__next__
     while True:
         t = [nxt(), nxt()][::swap and -1 or 1]
         if dims == 3:
@@ -3774,7 +3774,7 @@
     # whitespace. We'll also handle comma separators.
     try:
         coords = list(_gen_georss_coords(value, swap, dims))
-        return {u'type': u'Point', u'coordinates': coords[0]}
+        return {'type': 'Point', 'coordinates': coords[0]}
     except (IndexError, ValueError):
         return None

@@ -3784,7 +3784,7 @@
     # whitespace. There must be at least two pairs.
     try:
         coords = list(_gen_georss_coords(value, swap, dims))
-        return {u'type': u'LineString', u'coordinates': coords}
+        return {'type': 'LineString', 'coordinates': coords}
     except (IndexError, ValueError):
         return None

@@ -3799,7 +3799,7 @@
         return None
     if len(ring) < 4:
         return None
-    return {u'type': u'Polygon', u'coordinates': (ring,)}
+    return {'type': 'Polygon', 'coordinates': (ring,)}

 def _parse_georss_box(value, swap=True, dims=2):
     # A bounding box is a rectangular region, often used to define the extents
@@ -3808,7 +3808,7 @@
     # first pair is the lower corner, the second is the upper corner.
     try:
         coords = list(_gen_georss_coords(value, swap, dims))
-        return {u'type': u'Box', u'coordinates': tuple(coords)}
+        return {'type': 'Box', 'coordinates': tuple(coords)}
     except (IndexError, ValueError):
         return None

@@ -3840,7 +3840,7 @@
     try:
         f = _open_resource(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers)
         data = f.read()
-    except Exception, e:
+    except Exception as e:
         result['bozo'] = 1
         result['bozo_exception'] = e
         data = None
@@ -3856,7 +3856,7 @@

     # lowercase all of the HTTP headers for comparisons per RFC 2616
     if 'headers' in result:
-        http_headers = dict((k.lower(), v) for k, v in result['headers'].items())
+        http_headers = dict((k.lower(), v) for k, v in list(result['headers'].items()))
     else:
         http_headers = {}

@@ -3865,7 +3865,7 @@
         if gzip and 'gzip' in http_headers.get('content-encoding', ''):
             try:
                 data = gzip.GzipFile(fileobj=_StringIO(data)).read()
-            except (IOError, struct.error), e:
+            except (IOError, struct.error) as e:
                 # IOError can occur if the gzip header is bad.
                 # struct.error can occur if the data is damaged.
                 result['bozo'] = 1
@@ -3878,29 +3878,29 @@
         elif zlib and 'deflate' in http_headers.get('content-encoding', ''):
             try:
                 data = zlib.decompress(data)
-            except zlib.error, e:
+            except zlib.error as e:
                 try:
                     # The data may have no headers and no checksum.
                     data = zlib.decompress(data, -15)
-                except zlib.error, e:
+                except zlib.error as e:
                     result['bozo'] = 1
                     result['bozo_exception'] = e

     # save HTTP headers
     if http_headers:
         if 'etag' in http_headers:
-            etag = http_headers.get('etag', u'')
-            if not isinstance(etag, unicode):
+            etag = http_headers.get('etag', '')
+            if not isinstance(etag, str):
                 etag = etag.decode('utf-8', 'ignore')
             if etag:
                 result['etag'] = etag
         if 'last-modified' in http_headers:
-            modified = http_headers.get('last-modified', u'')
+            modified = http_headers.get('last-modified', '')
             if modified:
                 result['modified'] = modified
                 result['modified_parsed'] = _parse_date(modified)
     if hasattr(f, 'url'):
-        if not isinstance(f.url, unicode):
+        if not isinstance(f.url, str):
             result['href'] = f.url.decode('utf-8', 'ignore')
         else:
             result['href'] = f.url
@@ -3915,7 +3915,7 @@

     # Stop processing if the server sent HTTP 304 Not Modified.
     if getattr(f, 'code', 0) == 304:
-        result['version'] = u''
+        result['version'] = ''
         result['debug_message'] = 'The feed has not changed since you last checked, ' + \
             'so the server sent no data.  This is a feature, not a bug!'
         return result
@@ -3929,12 +3929,12 @@
     result['version'], data, entities = replace_doctype(data)

     # Ensure that baseuri is an absolute URI using an acceptable URI scheme.
-    contentloc = http_headers.get('content-location', u'')
-    href = result.get('href', u'')
+    contentloc = http_headers.get('content-location', '')
+    href = result.get('href', '')
     baseuri = _makeSafeAbsoluteURI(href, contentloc) or _makeSafeAbsoluteURI(contentloc) or href

     baselang = http_headers.get('content-language', None)
-    if not isinstance(baselang, unicode) and baselang is not None:
+    if not isinstance(baselang, str) and baselang is not None:
         baselang = baselang.decode('utf-8', 'ignore')

     if not _XML_AVAILABLE:
@@ -3955,7 +3955,7 @@
         source.setByteStream(_StringIO(data))
         try:
             saxparser.parse(source)
-        except xml.sax.SAXException, e:
+        except xml.sax.SAXException as e:
             result['bozo'] = 1
             result['bozo_exception'] = feedparser.exc or e
             use_strict_parser = 0
--- feedparser/feedparsertest.py	(original)
+++ feedparser/feedparsertest.py	(refactored)
@@ -40,13 +40,14 @@
 import threading
 import time
 import unittest
-import urllib
+import urllib.request, urllib.parse, urllib.error
 import warnings
 import zlib
-import BaseHTTPServer
-import SimpleHTTPServer
+import http.server
+import http.server

 import feedparser
+from functools import reduce

 if not feedparser._XML_AVAILABLE:
     sys.stderr.write('No XML parsers available, unit testing can not proceed\n')
@@ -69,7 +70,7 @@
 _PORT = 8097 # not really configurable, must match hardcoded port in tests
 _HOST = '127.0.0.1' # also not really configurable

-class FeedParserTestRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):
+class FeedParserTestRequestHandler(http.server.SimpleHTTPRequestHandler):
     headers_re = re.compile(_s2bytes(r"^Header:\s+([^:]+):(.+)$"), re.MULTILINE)

     def send_head(self):
@@ -86,7 +87,7 @@
             self.send_response(304)
             self.send_header('Content-type', 'text/xml')
             self.end_headers()
-            return feedparser._StringIO(u''.encode('utf-8'))
+            return feedparser._StringIO(''.encode('utf-8'))
         path = self.translate_path(self.path)
         # the compression tests' filenames determine the header sent
         if self.path.startswith('/tests/compression'):
@@ -108,7 +109,7 @@
         headers.setdefault('Content-type', self.guess_type(path))
         self.send_header("Content-type", headers['Content-type'])
         self.send_header("Content-Length", str(os.stat(f.name)[6]))
-        for k, v in headers.items():
+        for k, v in list(headers.items()):
             if k not in ('Status', 'Content-type'):
                 self.send_header(k, v)
         self.end_headers()
@@ -126,7 +127,7 @@
         self.ready = threading.Event()

     def run(self):
-        self.httpd = BaseHTTPServer.HTTPServer((_HOST, _PORT), FeedParserTestRequestHandler)
+        self.httpd = http.server.HTTPServer((_HOST, _PORT), FeedParserTestRequestHandler)
         self.ready.set()
         while self.requests:
             self.httpd.handle_request()
@@ -143,7 +144,7 @@

 def everythingIsUnicode(d):
     """Takes a dictionary, recursively verifies that every value is unicode"""
-    for k, v in d.iteritems():
+    for k, v in d.items():
         if isinstance(v, dict) and k != 'headers':
             if not everythingIsUnicode(v):
                 return False
@@ -163,9 +164,9 @@
     try:
         if not eval(evalString, globals(), env):
             failure=(msg or 'not eval(%s) \nWITH env(%s)' % (evalString, pprint.pformat(env)))
-            raise self.failureException, failure
+            raise self.failureException(failure)
         if not everythingIsUnicode(env):
-            raise self.failureException, "not everything is unicode \nWITH env(%s)" % (pprint.pformat(env), )
+            raise self.failureException("not everything is unicode \nWITH env(%s)" % (pprint.pformat(env), ))
     except SyntaxError:
         # Python 3 doesn't have the `u""` syntax, so evalString needs to be modified,
         # which will require the failure message to be updated
@@ -173,7 +174,7 @@
         evalString = re.sub(unicode2_re, _s2bytes(' "'), evalString)
         if not eval(evalString, globals(), env):
             failure=(msg or 'not eval(%s) \nWITH env(%s)' % (evalString, pprint.pformat(env)))
-            raise self.failureException, failure
+            raise self.failureException(failure)

 class BaseTestCase(unittest.TestCase):
     failUnlessEval = failUnlessEval
@@ -187,10 +188,10 @@
         warnings.filterwarnings('error')

         d = feedparser.FeedParserDict()
-        d['published'] = u'pub string'
-        d['published_parsed'] = u'pub tuple'
-        d['updated'] = u'upd string'
-        d['updated_parsed'] = u'upd tuple'
+        d['published'] = 'pub string'
+        d['published_parsed'] = 'pub tuple'
+        d['updated'] = 'upd string'
+        d['updated_parsed'] = 'upd tuple'
         # Ensure that `updated` doesn't map to `published` when it exists
         self.assertTrue('published' in d)
         self.assertTrue('published_parsed' in d)
@@ -202,8 +203,8 @@
         self.assertEqual(d['updated_parsed'], 'upd tuple')

         d = feedparser.FeedParserDict()
-        d['published'] = u'pub string'
-        d['published_parsed'] = u'pub tuple'
+        d['published'] = 'pub string'
+        d['published_parsed'] = 'pub tuple'
         # Ensure that `updated` doesn't actually exist
         self.assertTrue('updated' not in d)
         self.assertTrue('updated_parsed' not in d)
@@ -226,8 +227,8 @@
             self.assertEqual(True, False)
         # Ensure that `updated` maps to `published`
         warnings.filterwarnings('ignore')
-        self.assertEqual(d['updated'], u'pub string')
-        self.assertEqual(d['updated_parsed'], u'pub tuple')
+        self.assertEqual(d['updated'], 'pub string')
+        self.assertEqual(d['updated_parsed'], 'pub tuple')
         warnings.resetwarnings()


@@ -235,7 +236,7 @@
     "Ensure that `everythingIsUnicode()` is working appropriately"
     def test_everything_is_unicode(self):
         self.assertTrue(everythingIsUnicode(
-            {'a': u'a', 'b': [u'b', {'c': u'c'}], 'd': {'e': u'e'}}
+            {'a': 'a', 'b': ['b', {'c': 'c'}], 'd': {'e': 'e'}}
         ))
     def test_not_everything_is_unicode(self):
         self.assertFalse(everythingIsUnicode({'a': _s2bytes('a')}))
@@ -273,10 +274,10 @@
         <feed><title type="html">&exponential3;</title></feed>"""
         doc = codecs.BOM_UTF16_BE + doc.encode('utf-16be')
         result = feedparser.parse(doc)
-        self.assertEqual(result['feed']['title'], u'&amp;exponential3')
+        self.assertEqual(result['feed']['title'], '&amp;exponential3')
     def test_gb2312_converted_to_gb18030_in_xml_encoding(self):
         # \u55de was chosen because it exists in gb18030 but not gb2312
-        feed = u'''<?xml version="1.0" encoding="gb2312"?>
+        feed = '''<?xml version="1.0" encoding="gb2312"?>
                   <feed><title>\u55de</title></feed>'''
         result = feedparser.parse(feed.encode('gb18030'), response_headers={
             'Content-Type': 'text/xml'
@@ -367,11 +368,11 @@
         r = feedparser._open_resource(sys.stdin, '', '', '', '', [], {})
         self.assertTrue(r is sys.stdin)
     def test_feed(self):
-        f = feedparser.parse(u'feed://localhost:8097/tests/http/target.xml')
-        self.assertEqual(f.href, u'http://localhost:8097/tests/http/target.xml')
+        f = feedparser.parse('feed://localhost:8097/tests/http/target.xml')
+        self.assertEqual(f.href, 'http://localhost:8097/tests/http/target.xml')
     def test_feed_http(self):
-        f = feedparser.parse(u'feed:http://localhost:8097/tests/http/target.xml')
-        self.assertEqual(f.href, u'http://localhost:8097/tests/http/target.xml')
+        f = feedparser.parse('feed:http://localhost:8097/tests/http/target.xml')
+        self.assertEqual(f.href, 'http://localhost:8097/tests/http/target.xml')
     def test_bytes(self):
         s = '<feed><item><title>text</title></item></feed>'.encode('utf-8')
         r = feedparser._open_resource(s, '', '', '', '', [], {})
@@ -381,17 +382,17 @@
         r = feedparser._open_resource(s, '', '', '', '', [], {})
         self.assertEqual(s.encode('utf-8'), r.read())
     def test_unicode_1(self):
-        s = u'<feed><item><title>text</title></item></feed>'
+        s = '<feed><item><title>text</title></item></feed>'
         r = feedparser._open_resource(s, '', '', '', '', [], {})
         self.assertEqual(s.encode('utf-8'), r.read())
     def test_unicode_2(self):
-        s = u'<feed><item><title>t\u00e9xt</title></item></feed>'
+        s = '<feed><item><title>t\u00e9xt</title></item></feed>'
         r = feedparser._open_resource(s, '', '', '', '', [], {})
         self.assertEqual(s.encode('utf-8'), r.read())

 class TestMakeSafeAbsoluteURI(unittest.TestCase):
     "Exercise the URI joining and sanitization code"
-    base = u'http://d.test/d/f.ext'
+    base = 'http://d.test/d/f.ext'
     def _mktest(rel, expect, doc):
         def fn(self):
             value = feedparser._makeSafeAbsoluteURI(self.base, rel)
@@ -401,14 +402,14 @@

     # make the test cases; the call signature is:
     # (relative_url, expected_return_value, test_doc_string)
-    test_abs = _mktest(u'https://s.test/', u'https://s.test/', 'absolute uri')
-    test_rel = _mktest(u'/new', u'http://d.test/new', 'relative uri')
-    test_bad = _mktest(u'x://bad.test/', u'', 'unacceptable uri protocol')
-    test_mag = _mktest(u'magnet:?xt=a', u'magnet:?xt=a', 'magnet uri')
+    test_abs = _mktest('https://s.test/', 'https://s.test/', 'absolute uri')
+    test_rel = _mktest('/new', 'http://d.test/new', 'relative uri')
+    test_bad = _mktest('x://bad.test/', '', 'unacceptable uri protocol')
+    test_mag = _mktest('magnet:?xt=a', 'magnet:?xt=a', 'magnet uri')

     def test_catch_ValueError(self):
         'catch ValueError in Python 2.7 and up'
-        uri = u'http://bad]test/'
+        uri = 'http://bad]test/'
         value1 = feedparser._makeSafeAbsoluteURI(uri)
         value2 = feedparser._makeSafeAbsoluteURI(self.base, uri)
         swap = feedparser.ACCEPTABLE_URI_SCHEMES
@@ -416,24 +417,24 @@
         value3 = feedparser._makeSafeAbsoluteURI(self.base, uri)
         feedparser.ACCEPTABLE_URI_SCHEMES = swap
         # Only Python 2.7 and up throw a ValueError, otherwise uri is returned
-        self.assertTrue(value1 in (uri, u''))
-        self.assertTrue(value2 in (uri, u''))
-        self.assertTrue(value3 in (uri, u''))
+        self.assertTrue(value1 in (uri, ''))
+        self.assertTrue(value2 in (uri, ''))
+        self.assertTrue(value3 in (uri, ''))

 class TestConvertToIdn(unittest.TestCase):
     "Test IDN support (unavailable in Jython as of Jython 2.5.2)"
     # this is the greek test domain
-    hostname = u'\u03c0\u03b1\u03c1\u03ac\u03b4\u03b5\u03b9\u03b3\u03bc\u03b1'
-    hostname += u'.\u03b4\u03bf\u03ba\u03b9\u03bc\u03ae'
+    hostname = '\u03c0\u03b1\u03c1\u03ac\u03b4\u03b5\u03b9\u03b3\u03bc\u03b1'
+    hostname += '.\u03b4\u03bf\u03ba\u03b9\u03bc\u03ae'
     def test_control(self):
-        r = feedparser._convert_to_idn(u'http://example.test/')
-        self.assertEqual(r, u'http://example.test/')
+        r = feedparser._convert_to_idn('http://example.test/')
+        self.assertEqual(r, 'http://example.test/')
     def test_idn(self):
-        r = feedparser._convert_to_idn(u'http://%s/' % (self.hostname,))
-        self.assertEqual(r, u'http://xn--hxajbheg2az3al.xn--jxalpdlp/')
+        r = feedparser._convert_to_idn('http://%s/' % (self.hostname,))
+        self.assertEqual(r, 'http://xn--hxajbheg2az3al.xn--jxalpdlp/')
     def test_port(self):
-        r = feedparser._convert_to_idn(u'http://%s:8080/' % (self.hostname,))
-        self.assertEqual(r, u'http://xn--hxajbheg2az3al.xn--jxalpdlp:8080/')
+        r = feedparser._convert_to_idn('http://%s:8080/' % (self.hostname,))
+        self.assertEqual(r, 'http://xn--hxajbheg2az3al.xn--jxalpdlp:8080/')

 class TestCompression(unittest.TestCase):
     "Test the gzip and deflate support in the HTTP code"
@@ -490,13 +491,13 @@
         self.assertEqual(f.status, 200)
         self.assertEqual(f.entries[0].title, 'title 304')
         # extract the etag and last-modified headers
-        e = [v for k, v in f.headers.items() if k.lower() == 'etag'][0]
-        mh = [v for k, v in f.headers.items() if k.lower() == 'last-modified'][0]
+        e = [v for k, v in list(f.headers.items()) if k.lower() == 'etag'][0]
+        mh = [v for k, v in list(f.headers.items()) if k.lower() == 'last-modified'][0]
         ms = f.updated
         mt = f.updated_parsed
         md = datetime.datetime(*mt[0:7])
-        self.assertTrue(isinstance(mh, basestring))
-        self.assertTrue(isinstance(ms, basestring))
+        self.assertTrue(isinstance(mh, str))
+        self.assertTrue(isinstance(ms, str))
         self.assertTrue(isinstance(mt, time.struct_time))
         self.assertTrue(isinstance(md, datetime.datetime))
         # test that sending back the etag results in a 304
@@ -538,112 +539,112 @@
         # On some systems this date string will trigger an OverflowError.
         # On Jython and x64 systems, however, it's interpreted just fine.
         try:
-            date = feedparser._parse_date_rfc822(u'Sun, 31 Dec 9999 23:59:59 -9999')
+            date = feedparser._parse_date_rfc822('Sun, 31 Dec 9999 23:59:59 -9999')
         except OverflowError:
             date = None
         self.assertTrue(date in (None, (10000, 1, 5, 4, 38, 59, 2, 5, 0)))

 date_tests = {
     feedparser._parse_date_greek: (
-        (u'', None), # empty string
-        (u'\u039a\u03c5\u03c1, 11 \u0399\u03bf\u03cd\u03bb 2004 12:00:00 EST', (2004, 7, 11, 17, 0, 0, 6, 193, 0)),
+        ('', None), # empty string
+        ('\u039a\u03c5\u03c1, 11 \u0399\u03bf\u03cd\u03bb 2004 12:00:00 EST', (2004, 7, 11, 17, 0, 0, 6, 193, 0)),
     ),
     feedparser._parse_date_hungarian: (
-        (u'', None), # empty string
-        (u'2004-j\u00falius-13T9:15-05:00', (2004, 7, 13, 14, 15, 0, 1, 195, 0)),
+        ('', None), # empty string
+        ('2004-j\u00falius-13T9:15-05:00', (2004, 7, 13, 14, 15, 0, 1, 195, 0)),
     ),
     feedparser._parse_date_iso8601: (
-        (u'', None), # empty string
-        (u'-0312', (2003, 12, 1, 0, 0, 0, 0, 335, 0)), # 2-digit year/month only variant
-        (u'031231', (2003, 12, 31, 0, 0, 0, 2, 365, 0)), # 2-digit year/month/day only, no hyphens
-        (u'03-12-31', (2003, 12, 31, 0, 0, 0, 2, 365, 0)), # 2-digit year/month/day only
-        (u'-03-12', (2003, 12, 1, 0, 0, 0, 0, 335, 0)), # 2-digit year/month only
-        (u'03335', (2003, 12, 1, 0, 0, 0, 0, 335, 0)), # 2-digit year/ordinal, no hyphens
-        (u'2003-12-31T10:14:55.1234Z', (2003, 12, 31, 10, 14, 55, 2, 365, 0)), # fractional seconds
+        ('', None), # empty string
+        ('-0312', (2003, 12, 1, 0, 0, 0, 0, 335, 0)), # 2-digit year/month only variant
+        ('031231', (2003, 12, 31, 0, 0, 0, 2, 365, 0)), # 2-digit year/month/day only, no hyphens
+        ('03-12-31', (2003, 12, 31, 0, 0, 0, 2, 365, 0)), # 2-digit year/month/day only
+        ('-03-12', (2003, 12, 1, 0, 0, 0, 0, 335, 0)), # 2-digit year/month only
+        ('03335', (2003, 12, 1, 0, 0, 0, 0, 335, 0)), # 2-digit year/ordinal, no hyphens
+        ('2003-12-31T10:14:55.1234Z', (2003, 12, 31, 10, 14, 55, 2, 365, 0)), # fractional seconds
         # Special case for Google's extra zero in the month
-        (u'2003-012-31T10:14:55+00:00', (2003, 12, 31, 10, 14, 55, 2, 365, 0)),
+        ('2003-012-31T10:14:55+00:00', (2003, 12, 31, 10, 14, 55, 2, 365, 0)),
     ),
     feedparser._parse_date_nate: (
-        (u'', None), # empty string
-        (u'2004-05-25 \uc624\ud6c4 11:23:17', (2004, 5, 25, 14, 23, 17, 1, 146, 0)),
+        ('', None), # empty string
+        ('2004-05-25 \uc624\ud6c4 11:23:17', (2004, 5, 25, 14, 23, 17, 1, 146, 0)),
     ),
     feedparser._parse_date_onblog: (
-        (u'', None), # empty string
-        (u'2004\ub144 05\uc6d4 28\uc77c  01:31:15', (2004, 5, 27, 16, 31, 15, 3, 148, 0)),
+        ('', None), # empty string
+        ('2004\ub144 05\uc6d4 28\uc77c  01:31:15', (2004, 5, 27, 16, 31, 15, 3, 148, 0)),
     ),
     feedparser._parse_date_perforce: (
-        (u'', None), # empty string
-        (u'Fri, 2006/09/15 08:19:53 EDT', (2006, 9, 15, 12, 19, 53, 4, 258, 0)),
+        ('', None), # empty string
+        ('Fri, 2006/09/15 08:19:53 EDT', (2006, 9, 15, 12, 19, 53, 4, 258, 0)),
     ),
     feedparser._parse_date_rfc822: (
-        (u'', None), # empty string
-        (u'Thu, 01 Jan 0100 00:00:01 +0100', (99, 12, 31, 23, 0, 1, 3, 365, 0)), # ancient date
-        (u'Thu, 01 Jan 04 19:48:21 GMT', (2004, 1, 1, 19, 48, 21, 3, 1, 0)), # 2-digit year
-        (u'Thu, 01 Jan 2004 19:48:21 GMT', (2004, 1, 1, 19, 48, 21, 3, 1, 0)), # 4-digit year
-        (u'Thu,  5 Apr 2012 10:00:00 GMT', (2012, 4, 5, 10, 0, 0, 3, 96, 0)), # 1-digit day
-        (u'Wed, 19 Aug 2009 18:28:00 Etc/GMT', (2009, 8, 19, 18, 28, 0, 2, 231, 0)), # etc/gmt timezone
-        (u'Wed, 19 Feb 2012 22:40:00 GMT-01:01', (2012, 2, 19, 23, 41, 0, 6, 50, 0)), # gmt+hh:mm timezone
-        (u'Mon, 13 Feb, 2012 06:28:00 UTC', (2012, 2, 13, 6, 28, 0, 0, 44, 0)), # extraneous comma
-        (u'Thu, 01 Jan 2004 00:00 GMT', (2004, 1, 1, 0, 0, 0, 3, 1, 0)), # no seconds
-        (u'Thu, 01 Jan 2004', (2004, 1, 1, 0, 0, 0, 3, 1, 0)), # no time
+        ('', None), # empty string
+        ('Thu, 01 Jan 0100 00:00:01 +0100', (99, 12, 31, 23, 0, 1, 3, 365, 0)), # ancient date
+        ('Thu, 01 Jan 04 19:48:21 GMT', (2004, 1, 1, 19, 48, 21, 3, 1, 0)), # 2-digit year
+        ('Thu, 01 Jan 2004 19:48:21 GMT', (2004, 1, 1, 19, 48, 21, 3, 1, 0)), # 4-digit year
+        ('Thu,  5 Apr 2012 10:00:00 GMT', (2012, 4, 5, 10, 0, 0, 3, 96, 0)), # 1-digit day
+        ('Wed, 19 Aug 2009 18:28:00 Etc/GMT', (2009, 8, 19, 18, 28, 0, 2, 231, 0)), # etc/gmt timezone
+        ('Wed, 19 Feb 2012 22:40:00 GMT-01:01', (2012, 2, 19, 23, 41, 0, 6, 50, 0)), # gmt+hh:mm timezone
+        ('Mon, 13 Feb, 2012 06:28:00 UTC', (2012, 2, 13, 6, 28, 0, 0, 44, 0)), # extraneous comma
+        ('Thu, 01 Jan 2004 00:00 GMT', (2004, 1, 1, 0, 0, 0, 3, 1, 0)), # no seconds
+        ('Thu, 01 Jan 2004', (2004, 1, 1, 0, 0, 0, 3, 1, 0)), # no time
         # Additional tests to handle Disney's long month names and invalid timezones
-        (u'Mon, 26 January 2004 16:31:00 AT', (2004, 1, 26, 20, 31, 0, 0, 26, 0)),
-        (u'Mon, 26 January 2004 16:31:00 ET', (2004, 1, 26, 21, 31, 0, 0, 26, 0)),
-        (u'Mon, 26 January 2004 16:31:00 CT', (2004, 1, 26, 22, 31, 0, 0, 26, 0)),
-        (u'Mon, 26 January 2004 16:31:00 MT', (2004, 1, 26, 23, 31, 0, 0, 26, 0)),
-        (u'Mon, 26 January 2004 16:31:00 PT', (2004, 1, 27, 0, 31, 0, 1, 27, 0)),
+        ('Mon, 26 January 2004 16:31:00 AT', (2004, 1, 26, 20, 31, 0, 0, 26, 0)),
+        ('Mon, 26 January 2004 16:31:00 ET', (2004, 1, 26, 21, 31, 0, 0, 26, 0)),
+        ('Mon, 26 January 2004 16:31:00 CT', (2004, 1, 26, 22, 31, 0, 0, 26, 0)),
+        ('Mon, 26 January 2004 16:31:00 MT', (2004, 1, 26, 23, 31, 0, 0, 26, 0)),
+        ('Mon, 26 January 2004 16:31:00 PT', (2004, 1, 27, 0, 31, 0, 1, 27, 0)),
         # Swapped month and day
-        (u'Thu Aug 30 2012 17:26:16 +0200', (2012, 8, 30, 15, 26, 16, 3, 243, 0)),
-        (u'Sun, 16 Dec 2012 1:2:3:4 GMT', None), # invalid time
-        (u'Sun, 16 zzz 2012 11:47:32 GMT', None), # invalid month
-        (u'Sun, Dec x 2012 11:47:32 GMT', None), # invalid day (swapped day/month)
+        ('Thu Aug 30 2012 17:26:16 +0200', (2012, 8, 30, 15, 26, 16, 3, 243, 0)),
+        ('Sun, 16 Dec 2012 1:2:3:4 GMT', None), # invalid time
+        ('Sun, 16 zzz 2012 11:47:32 GMT', None), # invalid month
+        ('Sun, Dec x 2012 11:47:32 GMT', None), # invalid day (swapped day/month)
         ('Sun, 16 Dec zz 11:47:32 GMT', None), # invalid year
         ('Sun, 16 Dec 2012 11:47:32 +zz:00', None), # invalid timezone hour
         ('Sun, 16 Dec 2012 11:47:32 +00:zz', None), # invalid timezone minute
         ('Sun, 99 Jun 2009 12:00:00 GMT', None), # out-of-range day
     ),
     feedparser._parse_date_asctime: (
-        (u'Sun Jan  4 16:29:06 2004', (2004, 1, 4, 16, 29, 6, 6, 4, 0)),
-        (u'Sun Jul 15 01:16:00 +0000 2012', (2012, 7, 15, 1, 16, 0, 6, 197, 0)),
+        ('Sun Jan  4 16:29:06 2004', (2004, 1, 4, 16, 29, 6, 6, 4, 0)),
+        ('Sun Jul 15 01:16:00 +0000 2012', (2012, 7, 15, 1, 16, 0, 6, 197, 0)),
     ),
     feedparser._parse_date_w3dtf: (
-        (u'', None), # empty string
-        (u'2003-12-31T10:14:55Z', (2003, 12, 31, 10, 14, 55, 2, 365, 0)), # UTC
-        (u'2003-12-31T10:14:55-08:00', (2003, 12, 31, 18, 14, 55, 2, 365, 0)), # San Francisco timezone
-        (u'2003-12-31T18:14:55+08:00', (2003, 12, 31, 10, 14, 55, 2, 365, 0)), # Tokyo timezone
-        (u'2007-04-23T23:25:47.538+10:00', (2007, 4, 23, 13, 25, 47, 0, 113, 0)), # fractional seconds
-        (u'2003-12-31', (2003, 12, 31, 0, 0, 0, 2, 365, 0)), # year/month/day only
-        (u'2003-12', (2003, 12, 1, 0, 0, 0, 0, 335, 0)), # year/month only
-        (u'2003', (2003, 1, 1, 0, 0, 0, 2, 1, 0)), # year only
+        ('', None), # empty string
+        ('2003-12-31T10:14:55Z', (2003, 12, 31, 10, 14, 55, 2, 365, 0)), # UTC
+        ('2003-12-31T10:14:55-08:00', (2003, 12, 31, 18, 14, 55, 2, 365, 0)), # San Francisco timezone
+        ('2003-12-31T18:14:55+08:00', (2003, 12, 31, 10, 14, 55, 2, 365, 0)), # Tokyo timezone
+        ('2007-04-23T23:25:47.538+10:00', (2007, 4, 23, 13, 25, 47, 0, 113, 0)), # fractional seconds
+        ('2003-12-31', (2003, 12, 31, 0, 0, 0, 2, 365, 0)), # year/month/day only
+        ('2003-12', (2003, 12, 1, 0, 0, 0, 0, 335, 0)), # year/month only
+        ('2003', (2003, 1, 1, 0, 0, 0, 2, 1, 0)), # year only
         # Special cases for rollovers in leap years
-        (u'2004-02-28T18:14:55-08:00', (2004, 2, 29, 2, 14, 55, 6, 60, 0)), # feb 28 in leap year
-        (u'2003-02-28T18:14:55-08:00', (2003, 3, 1, 2, 14, 55, 5, 60, 0)), # feb 28 in non-leap year
-        (u'2000-02-28T18:14:55-08:00', (2000, 2, 29, 2, 14, 55, 1, 60, 0)), # feb 28 in leap year on century divisible by 400
+        ('2004-02-28T18:14:55-08:00', (2004, 2, 29, 2, 14, 55, 6, 60, 0)), # feb 28 in leap year
+        ('2003-02-28T18:14:55-08:00', (2003, 3, 1, 2, 14, 55, 5, 60, 0)), # feb 28 in non-leap year
+        ('2000-02-28T18:14:55-08:00', (2000, 2, 29, 2, 14, 55, 1, 60, 0)), # feb 28 in leap year on century divisible by 400
         # Out-of-range times
-        (u'9999-12-31T23:59:59-99:99', None), # Date is out-of-range
-        (u'2003-12-31T25:14:55Z', None), # invalid (25 hours)
-        (u'2003-12-31T10:61:55Z', None), # invalid (61 minutes)
-        (u'2003-12-31T10:14:61Z', None), # invalid (61 seconds)
+        ('9999-12-31T23:59:59-99:99', None), # Date is out-of-range
+        ('2003-12-31T25:14:55Z', None), # invalid (25 hours)
+        ('2003-12-31T10:61:55Z', None), # invalid (61 minutes)
+        ('2003-12-31T10:14:61Z', None), # invalid (61 seconds)
         # Invalid formats
-        (u'22013', None), # Year is too long
-        (u'013', None), # Year is too short
-        (u'2013-01-27-01', None), # Date has to many parts
-        (u'2013-01-28T11:30:00-06:00Textra', None), # Too many 't's
+        ('22013', None), # Year is too long
+        ('013', None), # Year is too short
+        ('2013-01-27-01', None), # Date has to many parts
+        ('2013-01-28T11:30:00-06:00Textra', None), # Too many 't's
         # Non-integer values
-        (u'2013-xx-27', None), # Date
-        (u'2013-01-28T09:xx:00Z', None), # Time
-        (u'2013-01-28T09:00:00+00:xx', None), # Timezone
+        ('2013-xx-27', None), # Date
+        ('2013-01-28T09:xx:00Z', None), # Time
+        ('2013-01-28T09:00:00+00:xx', None), # Timezone
         # MSSQL-style dates
-        (u'2004-07-08 23:56:58 -00:20', (2004, 7, 9, 0, 16, 58, 4, 191, 0)), # with timezone
-        (u'2004-07-08 23:56:58', (2004, 7, 8, 23, 56, 58, 3, 190, 0)), # without timezone
-        (u'2004-07-08 23:56:58.0', (2004, 7, 8, 23, 56, 58, 3, 190, 0)), # with fractional second
+        ('2004-07-08 23:56:58 -00:20', (2004, 7, 9, 0, 16, 58, 4, 191, 0)), # with timezone
+        ('2004-07-08 23:56:58', (2004, 7, 8, 23, 56, 58, 3, 190, 0)), # without timezone
+        ('2004-07-08 23:56:58.0', (2004, 7, 8, 23, 56, 58, 3, 190, 0)), # with fractional second
     )
 }

 def make_date_test(f, s, t):
     return lambda self: self._check_date(f, s, t)

-for func, items in date_tests.iteritems():
+for func, items in date_tests.items():
     for i, (dtstring, dttuple) in enumerate(items):
         uniqfunc = make_date_test(func, dtstring, dttuple)
         setattr(TestDateParsers, 'test_%s_%02i' % (func.__name__, i), uniqfunc)
@@ -658,17 +659,17 @@
         fn.__doc__ = doc
         return fn

-    test_text_1 = _mktest(u'plain text', False, u'plain text')
-    test_text_2 = _mktest(u'2 < 3', False, u'plain text with angle bracket')
-    test_html_1 = _mktest(u'<a href="">a</a>', True, u'anchor tag')
-    test_html_2 = _mktest(u'<i>i</i>', True, u'italics tag')
-    test_html_3 = _mktest(u'<b>b</b>', True, u'bold tag')
-    test_html_4 = _mktest(u'<code>', False, u'allowed tag, no end tag')
-    test_html_5 = _mktest(u'<rss> .. </rss>', False, u'disallowed tag')
-    test_entity_1 = _mktest(u'AT&T', False, u'corporation name')
-    test_entity_2 = _mktest(u'&copy;', True, u'named entity reference')
-    test_entity_3 = _mktest(u'&#169;', True, u'numeric entity reference')
-    test_entity_4 = _mktest(u'&#xA9;', True, u'hex numeric entity reference')
+    test_text_1 = _mktest('plain text', False, 'plain text')
+    test_text_2 = _mktest('2 < 3', False, 'plain text with angle bracket')
+    test_html_1 = _mktest('<a href="">a</a>', True, 'anchor tag')
+    test_html_2 = _mktest('<i>i</i>', True, 'italics tag')
+    test_html_3 = _mktest('<b>b</b>', True, 'bold tag')
+    test_html_4 = _mktest('<code>', False, 'allowed tag, no end tag')
+    test_html_5 = _mktest('<rss> .. </rss>', False, 'disallowed tag')RefactoringTool: Files that were modified:
RefactoringTool: feedparser/feedparser.py
RefactoringTool: feedparser/feedparsertest.py

+    test_entity_1 = _mktest('AT&T', False, 'corporation name')
+    test_entity_2 = _mktest('&copy;', True, 'named entity reference')
+    test_entity_3 = _mktest('&#169;', True, 'numeric entity reference')
+    test_entity_4 = _mktest('&#xA9;', True, 'hex numeric entity reference')

 #---------- additional api unit tests, not backed by files

@@ -694,7 +695,7 @@
         except ImportError:
             pass
         else:
-            doc = u"<feed>&illformed_charref</feed>".encode('utf8')
+            doc = "<feed>&illformed_charref</feed>".encode('utf8')
             # Importing lxml.etree currently causes libxml2 to
             # throw SAXException instead of SAXParseException.
             feedparser.parse(feedparser._StringIO(doc))
@@ -755,9 +756,9 @@
         skipUnless = '1'
     search_results = desc_re.search(data)
     if not search_results:
-        raise RuntimeError, "can't parse %s" % xmlfile
-    description, evalString = map(lambda s: s.strip(), list(search_results.groups()))
-    description = xmlfile + ": " + unicode(description, 'utf8')
+        raise RuntimeError("can't parse %s" % xmlfile)
+    description, evalString = [s.strip() for s in list(search_results.groups())]
+    description = xmlfile + ": " + str(description, 'utf8')
     return description, evalString, skipUnless

 def buildTestCase(xmlfile, description, evalString):
@@ -770,7 +771,7 @@
     "Read the files in the tests/ directory, dynamically add tests to the " \
     "TestCases above, spawn the HTTP server, and run the test suite"
     if sys.argv[1:]:
-        allfiles = filter(lambda s: s.endswith('.xml'), reduce(operator.add, map(glob.glob, sys.argv[1:]), []))
+        allfiles = [s for s in reduce(operator.add, list(map(glob.glob, sys.argv[1:])), []) if s.endswith('.xml')]
         wellformedfiles = illformedfiles = encodingfiles = entitiesfiles = microformatfiles = []
         sys.argv = [sys.argv[0]] #+ sys.argv[2:]
     else:
@@ -865,7 +866,7 @@
                 # quite flaky.  Just what you want in a testing framework, no?
                 httpd.requests = 0
                 if httpd.ready:
-                    urllib.urlopen('http://127.0.0.1:8097/tests/wellformed/rss/aaa_wellformed.xml').read()
+                    urllib.request.urlopen('http://127.0.0.1:8097/tests/wellformed/rss/aaa_wellformed.xml').read()
             httpd.join(0)

 if __name__ == "__main__":
==> Sources are ready.
Copied source directory to /sources/python-feedparser
===> sudo -u tuscan PATH=/tmp/toolchain/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin CC=arm-linux-androideabi-gcc CXX=arm-linux-androideabi-g++ SYSROOT=/build/android-ndk-r10e/platforms/android-21/arch-arm makepkg --noextract --syncdeps --skipinteg --skippgpcheck --skipchecksums --noconfirm --nocolor --log --noprogressbar --nocheck
==> Making package: python-feedparser 5.2.1-1 (Sat Nov 14 01:11:37 UTC 2015)
==> Checking runtime dependencies...
==> Checking buildtime dependencies...
==> WARNING: Using existing $srcdir/ tree
==> Starting build()...
  File "/usr/lib/python3.5/compileall.py", line 49
    yield from _walk_dir(fullname, ddir=dfile,
             ^
SyntaxError: invalid syntax
==> ERROR: A failure occurred in build().
    Aborting...
===> Printing config.logs
===> Finished printing config.logs></pre></code>
        </body></html>
